{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 10: Feature expansion\n",
    "\n",
    "## Summary and general instructions\n",
    "\n",
    "**Calculate and work with textual features beyond token unigram counts to predict volume publication dates.**\n",
    "\n",
    "We're going to work this week with **regression** rather than classification. This means that, rather than trying to predict a class label for each text, we'll try to estimate a continuous value for each text (in this case, the date of first publication of the book). We're also going to expand our feature set beyond unigram token counts.\n",
    "\n",
    "Despite the move from classification to regression, the general `sklearn` workflow is similar. You'll still create features using a vectorizer of some sort, you'll still set up a predictor object (now a regressor rather than a classifier), you'll still fit your predictor to your feature data, and you'll still produce a vector of predictions (now in the form of numbers rather than discreet labels). You can still score and cross-validate your results, but now by measuring the coefficient of determination, $R^2$, rather than $F_1$.\n",
    "\n",
    "We'll walk you through parts of this below. Good luck!\n",
    "\n",
    "## Imports and setup\n",
    "\n",
    "Recall that you can install SpaCy, if you haven't done so already, by running the following two lines in a code cell:\n",
    "\n",
    "```\n",
    "!conda install -c conda-forge spacy spacy-lookups-data -y\n",
    "!python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from   collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from   sklearn.feature_extraction import DictVectorizer\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from   sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from   sklearn.linear_model import LinearRegression\n",
    "from   sklearn.model_selection import cross_val_score\n",
    "from   sklearn.preprocessing import StandardScaler\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working texts\n",
    "# 10% samples of 40 US and GB novels\n",
    "base_url = 'https://raw.githubusercontent.com/centre-for-humanities-computing/NER_workshop/master/texts_short/'\n",
    "filenames = [\n",
    "    'A-Alcott-Little_Women-1868-F.txt',\n",
    "    'A-Cather-Antonia-1918-F.txt',\n",
    "    'A-Chesnutt-Marrow-1901-M.txt',\n",
    "    'A-Chopin-Awakening-1899-F.txt',\n",
    "    'A-Crane-Maggie-1893-M.txt',\n",
    "    'A-Davis-Life_Iron_Mills-1861-F.txt',\n",
    "    'A-Dreiser-Sister_Carrie-1900-M.txt',\n",
    "    'A-Freeman-Pembroke-1894-F.txt',\n",
    "    'A-Gilman-Herland-1915-F.txt',\n",
    "    'A-Harper-Iola_Leroy-1892-F.txt',\n",
    "    'A-Hawthorne-Scarlet_Letter-1850-M.txt',\n",
    "    'A-Howells-Silas_Lapham-1885-M.txt',\n",
    "    'A-James-Golden_Bowl-1904-M.txt',\n",
    "    'A-Jewett-Pointed_Firs-1896-F.txt',\n",
    "    'A-London-Call_Wild-1903-M.txt',\n",
    "    'A-Melville-Moby_Dick-1851-M.txt',\n",
    "    'A-Norris-Pit-1903-M.txt',\n",
    "    'A-Stowe-Uncle_Tom-1852-F.txt',\n",
    "    'A-Twain-Huck_Finn-1885-M.txt',\n",
    "    'A-Wharton-Age_Innocence-1920-F.txt',\n",
    "    'B-Austen-Pride_Prejudice-1813-F.txt',\n",
    "    'B-Bronte_C-Jane_Eyre-1847-F.txt',\n",
    "    'B-Bronte_E-Wuthering_Heights-1847-F.txt',\n",
    "    'B-Burney-Evelina-1778-F.txt',\n",
    "    'B-Conrad-Heart_Darkness-1902-M.txt',\n",
    "    'B-Dickens-Bleak_House-1853-M.txt',\n",
    "    'B-Disraeli-Sybil-1845-M.txt',\n",
    "    'B-Eliot-Middlemarch-1869-F.txt',\n",
    "    'B-Forster-Room_View-1908-M.txt',\n",
    "    'B-Gaskell-North_South-1855-F.txt',\n",
    "    'B-Gissing-Grub_Street-1893-M.txt',\n",
    "    'B-Hardy-Tess-1891-M.txt',\n",
    "    'B-Mitford-Our_Village-1826-F.txt',\n",
    "    'B-Radcliffe-Mysteries_Udolpho-1794-F.txt',\n",
    "    'B-Shelley-Frankenstein-1818-F.txt',\n",
    "    'B-Stevenson-Treasure_Island-1883-M.txt',\n",
    "    'B-Thackeray-Vanity_Fair-1848-M.txt',\n",
    "    'B-Trollope-Live_Now-1875-M.txt',\n",
    "    'B-Wells-Time_Machine-1895-M.txt',\n",
    "    'B-Woolf-Mrs_Dalloway-1925-F.txt'\n",
    "]\n",
    "\n",
    "# Randomize file order to avoid date sequence issues (B novels published earlier than A, on average)\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# Use a dictionary to store full texts, keyed to file name\n",
    "lit_texts = {}\n",
    "\n",
    "# Pull texts from public GitHub (not our class site)\n",
    "for f in filenames:\n",
    "    lit_texts[f] = requests.get(base_url+f).text\n",
    "    \n",
    "# Gold labels (pub dates), parsed from filenames\n",
    "y = [int(file.split('-')[3]) for file in filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple vectorizer (5 points)\n",
    "\n",
    "* Use the vectorizer below to create a feature matrix of normalized token counts for the 12 most frequently occurring words in the corpus. Your output matrix should have shape (40, 12). This is our **baseline** case. (3 points)\n",
    "* Scale the feature matrix using a `StandardScaler()` object. (1 point)\n",
    "* Print the resulting feature matrix shape and mean value (using `np.mean()`; it should be close to zero). (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer (freebie)\n",
    "simple_vectorizer = TfidfVectorizer(\n",
    "    input = 'content',\n",
    "    encoding = 'utf-8',\n",
    "    strip_accents = 'unicode',\n",
    "    lowercase = True,\n",
    "    min_df = 0.5,\n",
    "    max_features = 12,\n",
    "    use_idf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize, scale, and print the shape and the mean of the scaled matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple linear regression (15 points)\n",
    "\n",
    "Use a `LinearRegression()` object to predict the publication date of each novel in the corpus.\n",
    "\n",
    "* Study the [`LinearRegression()` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* Note that the gold values, `y`, have already been supplied for you (see setup section above)\n",
    "* Broadly speaking, you want to train your regressor by calling its `.fit()` method on your scaled input feature matrix and your gold labels, then use the fitted regressor to `.predict()` new y values given the same input. (5 points for fitting, 5 points for predicting)\n",
    "    * This will provide a misleading sense of predictive performance, because training and testing on the same data encourages overfitting. We'll deal with this problem below.\n",
    "* Plot your predicted publication dates as a function of the true publication dates. Nothing fancy needed, though +1 point if you do this with Seaborn's `regplot` to produce a line of best fit and a confidence interval. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict using LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_pred vs. y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score your baseline regressor (15 points)\n",
    "\n",
    "* First, score your trained regressor by calling its `.score()` method on the full input matrix and gold standard values. This calculates $R^2$, the coefficient of determination, which is an appropriate scoring metric for a regression problem. Print your score. It'll be somewhere near 0.5. (3 points)\n",
    "* Second, use `sklearn`'s `cross_val_score` function to calculate a proper, non-overfitted $R^2$. Print your result. (7 points)\n",
    "    * Use `scoring='r2'` and `cv=5` as parameters for your `cross_val_score`. \n",
    "    * See the [`cross_val_score` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) for implementation details.\n",
    "    * Your cross-validated $R^2$ value will be around -1.5 or -2.0, which is *terrible*. It means that just guessing the mean publication date as your answer for *every* book would perform better than this regressor.\n",
    "* We'll calculate these same scores several more times (using new feature data) in subsequent questions. Wrap up the calculations as a function, `compare_scores`, that takes a feature matrix and a vector of gold values, fits a `LinearRegressor` object, and prints both versions (na√Øve, overfitted `.score()` and 5-fold cross-validated) of the $R^2$ score. (5 points)\n",
    "    * Call this function on your data to confirm that it works and that it produces the same results as the ones your just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2, overfitted, using .score() on all data\n",
    "\n",
    "# Proper cross-validated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare true to naive regressor performance\n",
    "def compare_scores(X, y):\n",
    "    '''\n",
    "    Takes a feature matrix and a set of gold labels.\n",
    "    Fits a LinearRegressor and prints a naive R^2 score and a cross-validated R^2 score.\n",
    "    Returns nothing.\n",
    "    '''\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call compare_scores on your full data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmas (20 points)\n",
    "\n",
    "* Write a function, `lemmatizer`, that uses spaCy to lemmatize an input string (10 points, of which 8 for the function and 2 for checking it on the sample text below)\n",
    "    * This function should take an arbitrary string of text as input and return a list of lemmatized tokens\n",
    "    * Your lemmatizer should remove punctuation and any \"pure\" whitespace tokens (look out in particular for `\\n\\n` pseudo-tokens)\n",
    "* Use your lemmatizer function as an option with the supplied vectorizer to produce a feature matrix of the 12 most frequently occurring lemmas in the corpus.\n",
    "* Scale the resulting features using a `StandardScaler` object. (5 points total for vectorizing and scaling)\n",
    "* Finally, use your `compare_scores` function to fit a linear regressor on the scaled lemma features and report both the \"na√Øve\" and the cross-validated $R^2$ scores, calculated as in the previous question. (5 points for scoring)\n",
    "    * Your $R^2$ values should be broadly similar to those calculated in the previous problem\n",
    "\n",
    "FYI, vectorization with the lemmatizer takes about two minutes on my laptop (90 seconds if I cut corners and call an existing global `nlp` object from within the `lemmatizer` function, rather than setting up a new one for each text). NLP is slow, even when it's fast. This is also why we're  working with 10% samples of the novels rather than full texts.\n",
    "\n",
    "You can check your lemmatizer by running it with the following string as input:\n",
    "\n",
    "```\n",
    "'''Her cats are \\n\\n   dancing faster, than the tallest dogs.'''\n",
    "```\n",
    "\n",
    "Your output should be:\n",
    "\n",
    "```\n",
    "['-PRON-', 'cat', 'be', 'dance', 'fast', 'than', 'the', 'tall', 'dog']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a preprocessor: tokenize and lemmatize as indicated\n",
    "def lemmatizer(text):\n",
    "    import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your lemmatizer\n",
    "test_string = '''Her cats are \\n\\n   dancing faster, than the tallest dogs.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer (freebie)\n",
    "lemma_vectorizer = TfidfVectorizer(\n",
    "    input = 'content',\n",
    "    encoding = 'utf-8',\n",
    "    strip_accents = 'unicode',\n",
    "    lowercase = True,\n",
    "    tokenizer = lemmatizer,\n",
    "    min_df = 0.5,\n",
    "    max_features = 12,\n",
    "    use_idf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize, scale, and score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entities and POS tags (20 points)\n",
    "\n",
    "* Use spaCy to count the number of entities and the number of tokens tagged with parts of speech of the indicated types (10 points)\n",
    "    * Store your counts for each novel in a `Counter()` object or other dictionary-like structure\n",
    "    * Store all of your counters in a list\n",
    "    * This will require about the same amount of runtime as did the previous, lemmatized vectorization\n",
    "* Use a `DictVectorizer` to transform your list of populated counters into a feature matrix (7 points)\n",
    "* Scale the resulting matrix (1 point)\n",
    "* Use the scaled matrix as input to your `compare_scores` function. Calculate and print both a na√Øve and a cross-validated $R^2$ value. (2 points)\n",
    "    * Your performance won't be great. The cross-validated $R^2$ will still be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities and POS tags\n",
    "valid_ent = ['PERSON', 'MONEY', 'TIME']\n",
    "valid_pos = ['ADJ', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'PRON', 'PROPN', 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize counts, scale, and score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined features (8 points)\n",
    "\n",
    "* Combine your scaled, lemmatized features with your scaled, entity/POS features using `np.hstack` to produce a single feature matrix with shape (40, 24) (6 points)\n",
    "* Use this combined feature set and your `compare_scores` function to calculate na√Øve and cross-validated $R^2$ scores for the same predition task as in the previous problems (2 points)\n",
    "    * Note that your na√Øve $R^2$ has increased, but your cross-validated score has gotten worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack features and score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select the best features (7 points)\n",
    "\n",
    "Our problem is that we're overfitting. \n",
    "\n",
    "* Combat this problem by using `SelectKBest` to reduce your combined feature set to just the **five most informative features** (as determined by `mutual_info_regression` score) (5 points)\n",
    "* Calculate, again, the na√Øve and cross-validated $R^2$ scores using the selected features and `compare_scores` (2 points)\n",
    "    * Note that the na√Øve score has gone *way* down, but the cross-validated score has improved (though we're still performing worse than just guessing the mean publication date across the board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 best features and score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Once more, from the top (10 points)\n",
    "\n",
    "Let's reset and see if we can get performance better than random guessing.\n",
    "\n",
    "* Vectorize as indicated below. Here, we lemmatize and keep all lemmas that occur in 2 or more documents, without a cap on the number of retained features. We also use IDF weighting. (1 point)\n",
    "* Print the shape of the resulting feature matrix (1 point)\n",
    "* Scale your features (1 point)\n",
    "* Select the five most-informative features as scored by `mutual_info_regression` (5 points)\n",
    "* Use `compare_scores` to print the naive and cross-validated $R^2$ scores for the linear regression classifier on the selected features (2 points)\n",
    "    * You should now be performing better than random! My cross-validated $R^2$ is about 0.2. Not great, but better than anything we've seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer (freebie)\n",
    "\n",
    "big_vectorizer = TfidfVectorizer(\n",
    "    input = 'content',\n",
    "    encoding = 'utf-8',\n",
    "    strip_accents = 'unicode',\n",
    "    lowercase = True,\n",
    "    tokenizer = lemmatizer,\n",
    "    min_df = 2,\n",
    "    use_idf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize, scale, select, and score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
