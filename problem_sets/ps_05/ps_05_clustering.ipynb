{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 5: Clustering\n",
    "\n",
    "## Description\n",
    "\n",
    "**The goal of this problem set is to compute and explore three different clusterings of historical court records.**\n",
    "\n",
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "from   sklearn.cluster import KMeans, SpectralClustering, DBSCAN, OPTICS, AgglomerativeClustering\n",
    "from   sklearn.datasets import make_blobs\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from   sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "# Our input text file\n",
    "old_bailey_file = os.path.join('..', '..', 'data', 'old_bailey', 'old_bailey.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions\n",
    "\n",
    "Creating visualizations of your data and pulling sample texts for individual comparison are important parts of assessing the performance of your model. To make these two tasks easier, I've provided a function to do each one.\n",
    "\n",
    "Note that the visualization function performs dimension reduction, so that we can plot high-dimensional text data in 2-D space. The clustering operations are performed on the original, high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_compare(X, labels, title, reduce=True, alpha=0.2):\n",
    "    '''\n",
    "    Takes an array of object data, a set of cluster labels, and a title string\n",
    "    Reduces dimensions to 2 and plots the clustering.\n",
    "    Returns nothing.\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from   sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "    if reduce:\n",
    "        # TruncatedSVD is fast and can handle sparse inputs\n",
    "        # PCA requires dense inputs; MDS is slow\n",
    "        coordinates = TruncatedSVD(n_components=2).fit_transform(X)\n",
    "    else:\n",
    "        # Optionally handle 2-D inputs\n",
    "        coordinates = X\n",
    "    \n",
    "    # Set up figure\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "    # Unlabeled data\n",
    "    plt.subplot(121) # 1x2 plot, position 1\n",
    "    plt.scatter(\n",
    "        coordinates[:, 0], \n",
    "        coordinates[:, 1], \n",
    "        alpha=alpha, # Set transparency so that we can see overlapping points\n",
    "        linewidths=0 # Get rid of marker outlines\n",
    "    )\n",
    "    plt.title(\"Unclustered data\")\n",
    "\n",
    "    # Labeled data\n",
    "    plt.subplot(122)\n",
    "    sns.scatterplot(\n",
    "        x=coordinates[:, 0], \n",
    "        y=coordinates[:, 1],\n",
    "        hue=labels,\n",
    "        alpha=alpha,\n",
    "        palette='viridis',\n",
    "        linewidth=0\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# Pull sample texts from each label set\n",
    "def pull_samples(texts, labels, n=3):\n",
    "    '''\n",
    "    Takes lists of texts and an array of labels, as well as number of samples to return per label.\n",
    "    Prints sample texts belonging to each label.\n",
    "    '''\n",
    "    texts_array = np.array(texts) # Make the input text list easily addressable by NumPy\n",
    "    for label in np.unique(labels): # Iterate over labels\n",
    "        print(\"Label:\", label)\n",
    "        sample_index = np.where(labels == label)[0] # Limit selection to current label\n",
    "        print(\"Number of texts in this cluster:\", len(sample_index), '\\n')\n",
    "        chosen = np.random.choice(sample_index, size=n) # Sample n texts with this label\n",
    "        for choice in chosen:\n",
    "            print(\"Sample text:\", choice)\n",
    "            print(texts_array[choice], '\\n') # Print each sampled text\n",
    "        print(\"###################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic example\n",
    "\n",
    "To see how clustering works in `scikit-learn` (it's really easy!), here's a synthetic example.\n",
    "\n",
    "Note that `X` and `y` are the conventional labels for an input data array and a vector of labels, respectively. You don't have to use these (and may well want to at least give your variables informative suffixes), but you'll see this notation a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500 # this many points per blob\n",
    "X, y = make_blobs(n_samples=n_samples) # Make data and true labels\n",
    "y_pred = KMeans(n_clusters=3).fit_predict(X) # Perform clustering\n",
    "\n",
    "plot_compare(X, y, 'True blobs', reduce=False)\n",
    "plot_compare(X, y_pred, 'k-Means labels', reduce=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Notice that all we needed was to construct input data, set up a clustering object, and call the `fit_predict` method on the input data.\n",
    "\n",
    "This is what you'll do for the graded portions to follow.\n",
    "\n",
    "## Old Bailey records\n",
    "\n",
    "We'll work with a set of 3,090 short text documents from the Old Bailey, the main criminal court of the city of London. These records are a small subset of the almost 200,000 total digitized records collected by [The Old Bailey Proceedings Online](https://www.oldbaileyonline.org/static/Project.jsp).\n",
    "\n",
    "Our versions of the records have had most names removed. We need to perform some preprocessing, then vectorize the documents.\n",
    "\n",
    "Note that the file with which we're working collects all the records into a single document. Individual cases are delimited with two newlines, hence we split on `'\\n\\n'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cases in as a list of strings\n",
    "with open(old_bailey_file, 'r') as f:\n",
    "    bailey = [doc for doc in f.read().split('\\n\\n')] # split on consecutive newlines\n",
    "print(\"Total documents:\", len(bailey))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Pre-reflection (10 points)\n",
    "\n",
    "Reflect briefly on what you expect to find in the data. What types of cases to you expect the court to have adjudicated? How many of each type might you expect? What might be the important differences between case types? How cleanly do you expect the different cases to be separated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorize (5 points)\n",
    "\n",
    "Using the vectorizer defined below, transform the input documents into a TFIDF-weighted document-term matrix. Store your vectorized output in a varaible named `X_bailey`.\n",
    "\n",
    "Note: This should take one line of your own code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome preprocessing to remove escaped characters in input\n",
    "def pre_proc(x):\n",
    "    '''\n",
    "    Takes a unicode string.\n",
    "    Lowercases, strips accents, and removes some escapes.\n",
    "    Returns a standardized version of the string.\n",
    "    '''\n",
    "    import unicodedata\n",
    "    return unicodedata.normalize('NFKD', x.replace(\"\\'\", \"'\").replace(\"\\ in\\ form\", \" inform\").lower().strip())\n",
    "\n",
    "# Set up vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    encoding='utf-8',\n",
    "    preprocessor=pre_proc,\n",
    "    min_df=2, # Note this\n",
    "    max_df=0.8, # This, too\n",
    "    binary=False,\n",
    "    norm='l2',\n",
    "    use_idf=True # And this\n",
    ")\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Get the dimensions of the doc-term matrix\n",
    "print(\"Matrix shape:\", X_bailey.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie: Display a document and its vectorized features\n",
    "doc = 1000 # Which document to use?\n",
    "print(\"A sample document:\\n\", bailey[doc], '\\n')\n",
    "print(\"The document's features:\\n\", sorted(vectorizer.inverse_transform(X_bailey[doc])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie: Check that we've retained gendered pronouns in our vectorization\n",
    "for pronoun in ['she', 'her', 'hers', 'he', 'him', 'his']:\n",
    "    print(pronoun, '\\t', pronoun in vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform *k*-Means clustering (15 points)\n",
    "\n",
    "Perform *k*-Means clustering with `n_clusters=3` on your vectorized data. See the synthetic example above for guidance and/or consult the scikit-learn [kMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) documentation.\n",
    "\n",
    "Specifically, you must:\n",
    "\n",
    "1. Perform the clustering and store your output labels in variable named `y_kmeans`\n",
    "1. Print the shape of your label vector (it should match the number of documents in the input data set)\n",
    "1. Plot the resulting clustering using the supplied `plot_compare` function\n",
    "\n",
    "You should be able to accomplish these tasks in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-Means clustering with n_clusters = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot k-Means results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze *k*-Means clustering results (15 points)\n",
    "\n",
    "Use the supplied `pull_samples` function to examine 5 documents from each cluster (5 points). Then, write a paragraph analyzing your results (10 points). \n",
    "\n",
    "Can you make sense of the clusters? Do the cases look like what you expected? Do the calculated clusters strike you as meaningfully distinct? Do you think justice was done in the cases you examined? Anything else that stands out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull k-Means samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your discussion of the *k*-Means results here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spectral clustering (30 points)\n",
    "\n",
    "*k*-Means is just one approach to clustering. Here, you'll produce produce a **Spectral clustering**, with cosine similarity, as a point of comparison. You may want to consult the scikit-learn [SpectralClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) documentation.\n",
    "\n",
    "You must:\n",
    "\n",
    "1. Set up a `SpectralClustering` object with `n_clusters=3` and `affinity='precomputed'`\n",
    "1. Calculate the pairwise `cosine_similarity` matrix on your vectorized input data\n",
    "1. Compute a spectral clustering on the cosine similarity matrix, storing the output labels in a variable named `y_spectral`\n",
    "1. Print the shape of your output label vector\n",
    "1. Plot your results using the `plot_compare` function\n",
    "1. Pull and print 5 sample cases belonging to each cluster using the `pull_samples` function\n",
    "1. Discuss the results of your spectral clustering, both on their own and in comparison to the *k*-Means results. About an honest aragraph should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your spectral clustering code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spectral results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull spectral samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your discussion of the spectral clustering results here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Any other clustering method (25 points)\n",
    "\n",
    "Perform the same steps as in the *k*-Means and Spectral clustering cases (set up with appropriate options, compute labels, print label vector dimensions, plot, pull samples, and discuss results), but using a different clustering method from among [those offered by scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#clustering). Say something brief about why you chose the method you did as part of your discussion.\n",
    "\n",
    "Note: You'll want to pay particular attention to data input formats and to any key parameters of your chosen clustering method. Just as Spectral clustering required an affinity (similarity) matrix as input, some methods want a precomputed distance or similarity matrix as input. Others are very sensitive to options (like epsilon in DBSCAN). Work with care and note in your discussion any choices that you feel were important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your discussion here**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
