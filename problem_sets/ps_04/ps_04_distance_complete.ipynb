{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 4: Textual distances (solution)\n",
    "\n",
    "## Description\n",
    "\n",
    "**The goal of this problem set is to calculate the distance between several texts and to examine the effects of different distance metrics and input preprocessing steps.**\n",
    "\n",
    "You'll use the techniques developed in this problem set in the next several problems sets as well.\n",
    "\n",
    "Pay attention to the `import` statements in the next cell. We're going to use `scikit-learn` for both vectorization and distance calculations. We could do these steps by hand (indeed, we'll calculate by hand the Euclidean distance between two trivial documents), but `scikit-learn` has several advantages:\n",
    "\n",
    "* It implements configurable preprocessing steps.\n",
    "* It's well-vetted, so is unlikely to contain arithmetic errors.\n",
    "* It integrates with standard Python machine learning workflows.\n",
    "\n",
    "OK, to work!\n",
    "\n",
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 My cat likes water. She likes water so much that she often falls in the sink. But she hates to be wet.\n",
      "1 The dog eats food all day. The dog is named spot, but has no spots. The dog is furry.\n",
      "2 The dog and the cat play together. When they are tired, they eat and drink and sleep.\n",
      "3 A dog and a cat meet another dog and cat. They play, but only for a short while. They are not friends.\n",
      "4 The bird and the snake run in the woods. They do nothing like what the others do. But their function words do.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from   sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from   sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "dev_text = '''\\\n",
    "My cat likes water. She likes water so much that she often falls in the sink. But she hates to be wet.\n",
    "The dog eats food all day. The dog is named spot, but has no spots. The dog is furry.\n",
    "The dog and the cat play together. When they are tired, they eat and drink and sleep.\n",
    "A dog and a cat meet another dog and cat. They play, but only for a short while. They are not friends.\n",
    "The bird and the snake run in the woods. They do nothing like what the others do. But their function words do.'''\n",
    "\n",
    "dev_docs = [doc for doc in dev_text.split('\\n')] # Documents are one-per-line\n",
    "\n",
    "# Print docs for reference\n",
    "for i, text in enumerate(dev_docs):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Judging similarity (5 points)\n",
    "\n",
    "Consider the very short documents in `dev_docs`. Which do you judge to be most similar? Least similar? Why? Answer these questions in a couple of sentences.\n",
    "\n",
    "**Your answers here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Vectorization and Euclidean distance by hand\n",
    "\n",
    "Consider two sentences (or \"sentences\"):\n",
    "\n",
    "* sent_1: \"apple red\"\n",
    "* sent_2: \"orange orange\"\n",
    "\n",
    "How far apart are these documents in three-dimensional Euclidean space?\n",
    "\n",
    "First, record the vector representation of each sentence, where the count of the word \"apple\" is dimension 1, \"red\" is dimension two, and \"orange\" is dimension three. Your vectors should look like `sent_1 = [x, y, z]` where x, y, and z are integers.\n",
    "\n",
    "### Vector answer (5 points)\n",
    "\n",
    "Write the two vectors in Markdown in the cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your vector representations here**\n",
    "\n",
    "* `sent_1 = [1,1,0]`\n",
    "* `sent_2 = [0,0,2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Euclidean distance between two points is the square root of the sum of the squared differences between the points in each dimension. As an equation:\n",
    "\n",
    "$dist = \\sqrt{a^2 + b^2 + c^2 + ... + n^2}$\n",
    "\n",
    "where *a*, *b*, *c*, ..., *n* are the differences between the points in each of the *n* dimensions of the vector space.\n",
    "\n",
    "### Euclidean distance answer (5 points)\n",
    "\n",
    "Write the Euclidean distance between `sent_1` and `sent_2` in the Markdown cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your hand-calculated Euclidean distance answer here**\n",
    "\n",
    "$dist = \\sqrt{1^2 + 1^2 + 2^2} = \\sqrt{6} = 2.449...$ (2.4 and variations are fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Vectorize and calculate distances with `scikit-learn`\n",
    "\n",
    "As you can see, hand vectorization becomes cumbersome in a hurry and is impossible for most real-world corpora. Let's see how to do with `scikit-learn`.\n",
    "\n",
    "We'll begin with `CountVectorizer`, which transforms input texts into vectors of word counts.\n",
    "\n",
    "Couple of things ot note:\n",
    "\n",
    "* The workflow here is:\n",
    "  1. Initialize a vectorizer object, then \n",
    "  1. Use the initialized vectorizer's `fit_transform` method to tranform your input texts into vectorized output.\n",
    "* The vectorizer has many options that control what it does. I've included some of the more useful ones. Make sure you understand what each one does.\n",
    "  * See also the `CountVectorizer` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "* I've also included some `print` statements that allow us to examine selected properties of the vector matrix and of the vectorizer itself. Make sure you understand what these are and how they are produced.\n",
    "\n",
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (5, 60)\n",
      "\n",
      "Feature labels: ['all', 'and', 'another', 'are', 'be', 'bird', 'but', 'cat', 'day', 'do', 'dog', 'drink', 'eat', 'eats', 'falls', 'food', 'for', 'friends', 'function', 'furry', 'has', 'hates', 'in', 'is', 'like', 'likes', 'meet', 'much', 'my', 'named', 'no', 'not', 'nothing', 'often', 'only', 'others', 'play', 'run', 'she', 'short', 'sink', 'sleep', 'snake', 'so', 'spot', 'spots', 'that', 'the', 'their', 'they', 'tired', 'to', 'together', 'water', 'wet', 'what', 'when', 'while', 'woods', 'words']\n",
      "\n",
      "Stopwords used: None\n",
      "\n",
      "Document 0 vector: [[0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 2 0 1 1 0 0 0 0 1 0 0\n",
      "  0 0 3 0 1 0 0 1 0 0 1 1 0 0 0 1 0 2 1 0 0 0 0 0]]\n",
      "\n",
      "Document 0 words: [array(['my', 'cat', 'likes', 'water', 'she', 'so', 'much', 'that',\n",
      "       'often', 'falls', 'in', 'the', 'sink', 'but', 'hates', 'to', 'be',\n",
      "       'wet'], dtype='<U8')]\n"
     ]
    }
   ],
   "source": [
    "# This is a freebie to show how it's done\n",
    "\n",
    "# Set up the vectorizer object\n",
    "count_vectorizer = CountVectorizer(\n",
    "    encoding='utf-8',\n",
    "    strip_accents='unicode', # or 'ascii' (faster but less robust)\n",
    "    lowercase=True,\n",
    "    stop_words=None, # or 'english'\n",
    "    min_df=1, # include words that occur in as few as a single document\n",
    "    max_df=1.0, # include words that occur in as many as all documents\n",
    "    binary=False # True = return 1 if word is present in document, else 0\n",
    ")\n",
    "\n",
    "# Perform vectorization\n",
    "count_matrix = count_vectorizer.fit_transform(dev_docs)\n",
    "\n",
    "# Print some useful info about our data\n",
    "print(\"Matrix shape:\", count_matrix.shape)\n",
    "print(\"\\nFeature labels:\", count_vectorizer.get_feature_names())\n",
    "print(\"\\nStopwords used:\", count_vectorizer.get_stop_words())\n",
    "print(\"\\nDocument 0 vector:\", count_matrix[0].toarray())\n",
    "print(\"\\nDocument 0 words:\", count_vectorizer.inverse_transform(count_matrix[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the pairwise distances between the documents\n",
    "\n",
    "Once we have our feature matrix from the previous step, it's a one-liner to calculate the pairwise distances between the objects. Note that the `euclidean_distances` and `cosine_distances` functions were imported from `sklearn` at the top of the notebook.\n",
    "\n",
    "We're operating naïvely here. We haven't normalized length or removed stopwords. And we think that Euclidean distances might not be very useful anyway. But it's a start.\n",
    "\n",
    "#### Naïve Euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 7.54983444, 7.28010989, 7.28010989, 7.74596669],\n",
       "       [7.54983444, 0.        , 6.4807407 , 6.78232998, 6.8556546 ],\n",
       "       [7.28010989, 6.4807407 , 0.        , 4.69041576, 6.40312424],\n",
       "       [7.28010989, 6.78232998, 4.69041576, 0.        , 7.54983444],\n",
       "       [7.74596669, 6.8556546 , 6.40312424, 7.54983444, 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freebie for illustration purposes\n",
    "euc = euclidean_distances(count_matrix)\n",
    "euc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output array is ordered by sentence number in both columns and rows. In other words, to find the distance between sentence 0 and sentence 1, you can count zero rows down and 1 over, or 1 down and zero over. Note that the distance values in those two matrix positions are identical (7.5498...).\n",
    "\n",
    "#### Naïve cosine distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.87690851, 0.89793793, 0.89793793, 0.8322949 ],\n",
       "       [0.87690851, 0.        , 0.69848866, 0.76549118, 0.64218678],\n",
       "       [0.89793793, 0.69848866, 0.        , 0.40740741, 0.6044226 ],\n",
       "       [0.89793793, 0.76549118, 0.40740741, 0.        , 0.84785485],\n",
       "       [0.8322949 , 0.64218678, 0.6044226 , 0.84785485, 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freebie, ditto\n",
    "cos = cosine_distances(count_matrix)\n",
    "cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Assess naïve distance results (10 points)\n",
    "\n",
    "Offer a few brief reflections on the distance metrics we just calculated. Which sentences are closest to one another in each case? Which are furthest apart? Do these results make any kind of sense? If you're surprised, where do you think the method (or your intuition) goes wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the impact of preprocessing and normalization\n",
    "\n",
    "**In the cell below, set up a new vectorizer that removes stopwords.** Keep the print statements from above. Then, in subsequent cells, calculate Euclidean and cosine distances on the new feature matrix. Finally, compare briefly the distance results without stopwords to those that retain stopwords.\n",
    "\n",
    "### Vectorize the dev documents, removing stopwords (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (5, 30)\n",
      "\n",
      "Feature labels: ['bird', 'cat', 'day', 'dog', 'drink', 'eat', 'eats', 'falls', 'food', 'friends', 'function', 'furry', 'hates', 'like', 'likes', 'meet', 'named', 'play', 'run', 'short', 'sink', 'sleep', 'snake', 'spot', 'spots', 'tired', 'water', 'wet', 'woods', 'words']\n",
      "\n",
      "Stopwords used: ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Document 0 vector: [[0 1 0 0 0 0 0 1 0 0 0 0 1 0 2 0 0 0 0 0 1 0 0 0 0 0 2 1 0 0]]\n",
      "\n",
      "Document 0 words: [array(['cat', 'likes', 'water', 'falls', 'sink', 'hates', 'wet'],\n",
      "      dtype='<U8')]\n"
     ]
    }
   ],
   "source": [
    "stopword_vectorizer = CountVectorizer(\n",
    "    encoding='utf-8',\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    min_df=1, # include words that occur in as few as a single document\n",
    "    max_df=1.0, # include words that occur in as many as all documents\n",
    "    binary=False # True = return 1 if word is present in document, else 0\n",
    ")\n",
    "count_matrix_stopsremoved = stopword_vectorizer.fit_transform(dev_docs)\n",
    "print(\"Matrix shape:\", count_matrix_stopsremoved.shape)\n",
    "print(\"\\nFeature labels:\", stopword_vectorizer.get_feature_names())\n",
    "print(\"\\nStopwords used:\", sorted(stopword_vectorizer.get_stop_words()))\n",
    "print(\"\\nDocument 0 vector:\", count_matrix_stopsremoved[0].toarray())\n",
    "print(\"\\nDocument 0 words:\", stopword_vectorizer.inverse_transform(count_matrix_stopsremoved[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances (5 points total)\n",
    "\n",
    "**Calculate (and display) Euclidean distances between your newly vectorized documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.38516481, 4.24264069, 4.58257569, 4.47213595],\n",
       "       [5.38516481, 0.        , 4.12310563, 4.        , 4.79583152],\n",
       "       [4.24264069, 4.12310563, 0.        , 3.        , 3.74165739],\n",
       "       [4.58257569, 4.        , 3.        , 0.        , 4.35889894],\n",
       "       [4.47213595, 4.79583152, 3.74165739, 4.35889894, 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Euclidean distances for the new, stopword-removed feature matrix\n",
    "euclidean_distances(count_matrix_stopsremoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate (and display) cosine distances on the same matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.89517152, 0.83987185, 1.        ],\n",
       "       [1.        , 0.        , 0.71652665, 0.5669873 , 1.        ],\n",
       "       [0.89517152, 0.71652665, 0.        , 0.45445527, 1.        ],\n",
       "       [0.83987185, 0.5669873 , 0.45445527, 0.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate cosine distances for the new, stopword-removed feature matrix\n",
    "cosine_distances(count_matrix_stopsremoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Compare these results to the previous ones (5 points)\n",
    "\n",
    "How do the distances calculated after removing stopwords compare to those obtained when retaining stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and try out TF-IDF weighting\n",
    "\n",
    "As noted in class, Euclidean distances are strongly influenced by document length. One way to minimize the impact of length is to *normalize* our vectors.\n",
    "\n",
    "To normalize our vectors, we need to switch to `TfidfVectorizer` (or else be prepared to do some math on our own). `TfidfVectorizer` has two helpful features that are not present in `CountVectorizer`:\n",
    "\n",
    "1. `TfidfVectorizer` applies selectable normalization to the calculated vectors. This prevents long documents from being far away from short documents simply because the documents contain different numbers of words.\n",
    "  1. There are two built-in normalization methods. `'l1'` produces vectors the elements of which sum to 1. `'l2'` produces vectors whose *squared* elements sum to 1.\n",
    "  1. One sees `l2` normalization used more often than `l1` for machine learning tasks. **In general**(!), *l2* produces better fits because it reduces the influence of outlying data points more aggressively, while *l1* produces sparser features (that is, feature vectors with more values that are close to zero). Sparse features can be desirable in some cases (feature selection and interpretation can be easier with sparse features, for instance).\n",
    "1. `TfidfVectorizer` allows us to apply [TF-IDF weighting](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), if desired. This is a method of downweighting words that appear in many documents (on the theory that words shared by many documents are less likely to tell us much about the distinctive content of any one document than are words that are not so widely shared). TF-IDF is a classic preprocessing step in many information retrieval tasks, though it's unclear how much it helps when assessing document similarity. Since it's a selectable flag in `TfidfVectorizer`, we can try it out and see what difference it makes.\n",
    "\n",
    "You can consult the `TfidfVectorizer` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) if you want details and options.\n",
    "\n",
    "### Vectorize (5 points)\n",
    "\n",
    "Vectorize the `dev_docs` with `TfidfVectorizer`, using `norm='l2'` and `use_idf=False` (that is, without TF-IDF weighting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (5, 30)\n",
      "\n",
      "Feature labels: ['bird', 'cat', 'day', 'dog', 'drink', 'eat', 'eats', 'falls', 'food', 'friends', 'function', 'furry', 'hates', 'like', 'likes', 'meet', 'named', 'play', 'run', 'short', 'sink', 'sleep', 'snake', 'spot', 'spots', 'tired', 'water', 'wet', 'woods', 'words']\n",
      "\n",
      "Document 0 vector: [[0.        0.2773501 0.        0.        0.        0.        0.\n",
      "  0.2773501 0.        0.        0.        0.        0.2773501 0.\n",
      "  0.5547002 0.        0.        0.        0.        0.        0.2773501\n",
      "  0.        0.        0.        0.        0.        0.5547002 0.2773501\n",
      "  0.        0.       ]]\n",
      "\n",
      "Document 0 words: [array(['cat', 'likes', 'water', 'falls', 'sink', 'hates', 'wet'],\n",
      "      dtype='<U8')]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize, norm on, idf off\n",
    "normalizing_vectorizer = TfidfVectorizer(\n",
    "    encoding='utf-8',\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    min_df=1, # 1 = a single document\n",
    "    max_df=1.0, # 1.0 = 100% = all documents,\n",
    "    binary=False, # True = return 1 if word is present in document, else 0\n",
    "    norm='l2',\n",
    "    use_idf=False\n",
    ")\n",
    "norm_matrix = normalizing_vectorizer.fit_transform(dev_docs)\n",
    "print(\"Matrix shape:\", norm_matrix.shape)\n",
    "print(\"\\nFeature labels:\", normalizing_vectorizer.get_feature_names())\n",
    "print(\"\\nDocument 0 vector:\", norm_matrix[0].toarray())\n",
    "print(\"\\nDocument 0 words:\", normalizing_vectorizer.inverse_transform(norm_matrix[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we're L2-normed. Sum of squared feature weights in each document should be 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared feature weights in document 0 sum to 1: True\n",
      "Squared feature weights in document 1 sum to 1: True\n",
      "Squared feature weights in document 2 sum to 1: True\n",
      "Squared feature weights in document 3 sum to 1: True\n",
      "Squared feature weights in document 4 sum to 1: True\n"
     ]
    }
   ],
   "source": [
    "# Verify l2-norm, freebie\n",
    "# You will need to change the input variable name below to match whatever you used inthe previous cell\n",
    "for i, vec in enumerate(norm_matrix.toarray()):\n",
    "    squared_features = [term**2 for term in vec]\n",
    "    print(f\"Squared feature weights in document {i} sum to 1:\", np.isclose(sum(squared_features), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Calculate distances for the normalized vectors and discuss the results (10 points total; 5 for distances, 5 for discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.41421356, 1.33803701, 1.29604926, 1.41421356],\n",
       "       [1.41421356, 0.        , 1.19710204, 1.06488243, 1.41421356],\n",
       "       [1.33803701, 1.19710204, 0.        , 0.953368  , 1.41421356],\n",
       "       [1.29604926, 1.06488243, 0.953368  , 0.        , 1.41421356],\n",
       "       [1.41421356, 1.41421356, 1.41421356, 1.41421356, 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Euclidean distances on l2-normed features with stopword removal\n",
    "euclidean_distances(norm_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.89517152, 0.83987185, 1.        ],\n",
       "       [1.        , 0.        , 0.71652665, 0.5669873 , 1.        ],\n",
       "       [0.89517152, 0.71652665, 0.        , 0.45445527, 1.        ],\n",
       "       [0.83987185, 0.5669873 , 0.45445527, 0.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine distances on l2-normed features with stopword removal\n",
    "cosine_distances(norm_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your discussion of the impact of normalization here.** Write a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Try TF-IDF (15 points total: 5 vectorization/5 distances/5 discussion)\n",
    "\n",
    "Set up a new vectorizer with normalization *and* TF-IDF weighting. Then calculate the Euclidean and cosine distance matrices and compare the results to the normalized but non-TF-IDF results immediately above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (5, 30)\n",
      "\n",
      "Feature labels: ['bird', 'cat', 'day', 'dog', 'drink', 'eat', 'eats', 'falls', 'food', 'friends', 'function', 'furry', 'hates', 'like', 'likes', 'meet', 'named', 'play', 'run', 'short', 'sink', 'sleep', 'snake', 'spot', 'spots', 'tired', 'water', 'wet', 'woods', 'words']\n",
      "\n",
      "Document 0 vector: [[0.         0.18981438 0.         0.         0.         0.\n",
      "  0.         0.28342702 0.         0.         0.         0.\n",
      "  0.28342702 0.         0.56685404 0.         0.         0.\n",
      "  0.         0.         0.28342702 0.         0.         0.\n",
      "  0.         0.         0.56685404 0.28342702 0.         0.        ]]\n",
      "\n",
      "Document 0 words: [array(['wet', 'hates', 'sink', 'falls', 'water', 'likes', 'cat'],\n",
      "      dtype='<U8')]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize, norm on, idf on\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    encoding='utf-8',\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    min_df=1, # 1 = a single document\n",
    "    max_df=1.0, # 1.0 = 100% = all documents,\n",
    "    binary=False, # True = return 1 if word is present in document, else 0\n",
    "    norm='l2',\n",
    "    use_idf=True\n",
    ")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(dev_docs)\n",
    "print(\"Matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"\\nFeature labels:\", tfidf_vectorizer.get_feature_names())\n",
    "print(\"\\nDocument 0 vector:\", tfidf_matrix[0].toarray())\n",
    "print(\"\\nDocument 0 words:\", tfidf_vectorizer.inverse_transform(tfidf_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.41421356, 1.37552185, 1.34573803, 1.41421356],\n",
       "       [1.41421356, 0.        , 1.28689221, 1.18231053, 1.41421356],\n",
       "       [1.37552185, 1.28689221, 0.        , 1.10832776, 1.41421356],\n",
       "       [1.34573803, 1.18231053, 1.10832776, 0.        , 1.41421356],\n",
       "       [1.41421356, 1.41421356, 1.41421356, 1.41421356, 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Euclidean distances\n",
    "euclidean_distances(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.94603018, 0.90550542, 1.        ],\n",
       "       [1.        , 0.        , 0.82804578, 0.69892909, 1.        ],\n",
       "       [0.94603018, 0.82804578, 0.        , 0.61419521, 1.        ],\n",
       "       [0.90550542, 0.69892909, 0.61419521, 0.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine distances\n",
    "cosine_distances(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your discussion of the impact of TF-IDF here.** Again, a couple of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally: Try a few novels (30 points total)\n",
    "\n",
    "Points breakdown:\n",
    "\n",
    "* 10 points for thoughtful vectorization settings\n",
    "* 5 points for calculating distances\n",
    "* 15 points for thoughtful dicussion of your results and their generalizability\n",
    "\n",
    "In the cell below is a list of five novels. They are:\n",
    "\n",
    "* Stephen Crane's *Maggie: A Girl of the Streets* (1893)\n",
    "* Theodore Dreiser's *Sister Carrie* (1900)\n",
    "* Charlotte Perkins Gilman's *Herland* (1915)\n",
    "* Jane Austen's *Pride and Prejudice* (1813)\n",
    "* George Eliot's *Middlemarch* (1869)\n",
    "\n",
    "There's reason to believe that some of these novels resemble one another, while others are quite dissimilar. \n",
    "\n",
    "* *Maggie*, *Sister Carrie*, and *Herland* are American novels published around the start of the twentieth century; Austen's and Eliot's novels are British and were published decades earlier.\n",
    "* The first two novels were written by men; the last three by women.\n",
    "* The first two books are Naturalist novels about the (mis)fortunes of young women in predatory cities. *Herland* is an early feminist utopia. The last two books are classic Romantic/Realist novels about British country life, with special emphasis on the aristocracy.\n",
    "\n",
    "Do the distance metrics we've seen so far reflect the differences we would expect to see between these texts?\n",
    "\n",
    "### Task: Calculate distances between these five texts. Discuss your results.\n",
    "\n",
    "* You may use any of the vectorization approaches and distance metrics explored above. \n",
    "* You must justify your decisions concerning parameters and techniques.\n",
    "* If you remove stopwords (you should, but there are different ways to do this, not all of which involve a fixed list of words), you must justify your choices. Unreflective reliance on a stock list of stopwords is not good enough.\n",
    "* Conclude your work with a discussion of your results.\n",
    "  * Did your results match your expectations? In what ways?\n",
    "  * What else might you try so as to improve your results?\n",
    "  * Can you make any more general observations about the approaches to document similarity that you've employed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel list\n",
    "novels = [\n",
    "    'A-Crane-Maggie-1893-M.txt',\n",
    "    'A-Dreiser-Sister_Carrie-1900-M.txt',\n",
    "    'A-Gilman-Herland-1915-F.txt',\n",
    "    'B-Austen-Pride_Prejudice-1813-F.txt',\n",
    "    'B-Eliot-Middlemarch-1869-F.txt'\n",
    "]\n",
    "\n",
    "# File location\n",
    "novel_path = os.path.join('..','..','data','texts')\n",
    "\n",
    "# Create list of full file paths\n",
    "files = [os.path.join(novel_path, novel) for novel in novels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (5, 5000)\n",
      "\n",
      "Euclidean:\n",
      " [[0.         1.28782107 1.36427803 1.38998101 1.3380117 ]\n",
      " [1.28782107 0.         1.11337328 1.14719942 0.96370704]\n",
      " [1.36427803 1.11337328 0.         1.25870711 1.11528058]\n",
      " [1.38998101 1.14719942 1.25870711 0.         0.97385512]\n",
      " [1.3380117  0.96370704 1.11528058 0.97385512 0.        ]]\n",
      "\n",
      "Cosine:\n",
      " [[0.         0.82924156 0.93062727 0.9660236  0.89513766]\n",
      " [0.82924156 0.         0.61980003 0.65803325 0.46436563]\n",
      " [0.93062727 0.61980003 0.         0.7921718  0.62192538]\n",
      " [0.9660236  0.65803325 0.7921718  0.         0.4741969 ]\n",
      " [0.89513766 0.46436563 0.62192538 0.4741969  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "stopwords = [\n",
    "     'a',\n",
    "     'above',\n",
    "     'am',\n",
    "     'an',\n",
    "     'and',\n",
    "     'are',\n",
    "     'at',\n",
    "     'be',\n",
    "     'been',\n",
    "     'being',\n",
    "     'but',\n",
    "     'had',\n",
    "     'has',\n",
    "     'have',\n",
    "     'in',\n",
    "     'is',\n",
    "     'of',\n",
    "     'on',\n",
    "     'out',\n",
    "     'said',\n",
    "     'saying',\n",
    "     'says',\n",
    "     'the',\n",
    "     'under',\n",
    "     'was',\n",
    "     'were',\n",
    "     'with'\n",
    "]\n",
    "\n",
    "novel_vectorizer = TfidfVectorizer(\n",
    "    input='filename', # Take input as file paths, not raw text\n",
    "    encoding='utf-8',\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words=stopwords,\n",
    "    min_df=2, # 1 = a single document\n",
    "    max_df=0.9, # 1.0 = 100% = all documents,\n",
    "    binary=False, # True = return 1 if word is present in document, else 0\n",
    "    norm='l2',\n",
    "    use_idf=False,\n",
    "    max_features=5000\n",
    ")\n",
    "novel_matrix = novel_vectorizer.fit_transform(files)\n",
    "print(\"Matrix shape:\", novel_matrix.shape)\n",
    "print(\"\\nEuclidean:\\n\", euclidean_distances(novel_matrix))\n",
    "print(\"\\nCosine:\\n\", cosine_distances(novel_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your discussion here.** Probably a couple of paragraphs.\n",
    "\n",
    "* How did you choose to vectorize?\n",
    "* How did you approach stopword removal?\n",
    "* How did you measure distances?\n",
    "* What did you find?\n",
    "* How might you improve your results?\n",
    "* Are you prepared to offer any generalizations about textual simlarity in novels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
