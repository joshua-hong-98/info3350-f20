{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 9: Statistics, feature selection, and feature importance\n",
    "\n",
    "## Summary\n",
    "\n",
    "Examine the differences between British and American fiction in the class-curated literary corpus. Apply statistical measures and calculate feature importance in a simple classifier.\n",
    "\n",
    "## Details\n",
    "\n",
    "You will work with a corpus of 131 volumes of fiction by British and American authors. These volumes are taken from the class corpus, so you'll need to download a copy of the texts from [Google Drive](https://drive.google.com/drive/folders/1lbeZiBAVCzjCWojCK8mfmELa-Q8FMNUm?usp=sharing) or from GitHub and save them somewhere on your machine.\n",
    "\n",
    "You have three tasks for this problem set, all of which depend on comparing British-authored to American-authored books:\n",
    "\n",
    "1. Calculate the mean frequency per 100,000 words, as well as the upper and lower bounds of a 95% confidence interval, for the terms `['color', 'honor', 'center', 'fish', 'person']` in each national subcorpus\n",
    "    1. Perform this calculation analyticaly, that is, using the observed sample means and standard deviations.\n",
    "    1. Calculate the same quantities via bootstrap, using 1,000 or more iterations.\n",
    "    1. In both cases, print your results in a tabular format.\n",
    "2. Perform a *t*-test to compare the mean frequency of each of these terms between British and American texts. Report the test statistic and *p*-value for each comparison. Note which means are significantly different at the *p*<0.05 level.\n",
    "3. Perform a logistic regression classification of each volume as British or American. \n",
    "    1. Your final features should be the 25 most informative (as measured by the mutual information criterion) token unigrams.\n",
    "    1. Report your 10-fold cross-validated F1 score before and after restricting your input features to the 25 most-informative token types.\n",
    "    1. Calculate the *importance* of the 25 top features for classification as measured by permutation importance.\n",
    "    \n",
    "* See code stubs below for step-by-step guidance. \n",
    "* Consult, too, the lecture notes on explainability and on statistics.\n",
    "* You'll likely also need to consult the scikit-learn documentation along the way.\n",
    "\n",
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metadata_file = 'amer_brit.csv'\n",
    "corpus_dir = os.path.join('..', '..', 'data', 'classcorpus')\n",
    "terms = ['color', 'honor', 'center', 'fish', 'person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata (5 points)\n",
    "\n",
    "Read the cleaned, minimal corpus metadata from disk (note the variable `metadata_file` defined in the previous cell). I'd suggest using Pandas, but you're welcome to use whatever method you prefer.\n",
    "\n",
    "Note that the format of the metadata file is:\n",
    "```\n",
    "filename,country,wordcount\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(metadata_file)\n",
    "# Read the corpus metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>country</th>\n",
       "      <th>wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little_Women_Alcott.txt</td>\n",
       "      <td>us</td>\n",
       "      <td>185902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename country      wc\n",
       "0  Little_Women_Alcott.txt      us  185902"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the metadata for one volume\n",
    "corpus.head(1)\n",
    "# corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count words and normalize (5 points)\n",
    "\n",
    "* Count the target words (indicated in the problem statement) in each volume. \n",
    "* Then, **normalize the count of each word type per 100,000 words** in each volume.\n",
    "*  I'd suggest using a `CountVectorizer` object, but again, you may approach this task however you like. \n",
    "* Make sure you lowercase the input tokens.\n",
    "* Use the word counts supplied in the metadata file for length normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and normalize the target terms in each volume as indicated\n",
    "#done during section\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(input= 'filename', vocabulary= terms)\n",
    "filenames = [os.path.join('..', '..', 'data', 'classcorpus', f) for f in list(corpus['filename'])]\n",
    "count = vectorizer.fit_transform(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.29627438  15.599617     0.           3.76542479  12.37211004]\n",
      " [  0.           0.           0.           9.24257128  13.86385692]\n",
      " [  0.           0.           4.47007286  26.82043717   8.94014572]\n",
      " [  0.           0.           0.           0.          25.95413904]\n",
      " [  0.           0.           0.           0.          38.42551454]\n",
      " [  0.           1.68693804   0.           0.84346902  58.19936234]\n",
      " [  0.           0.           0.           4.11373659  41.13736589]\n",
      " [  0.           0.           0.           0.          21.31207141]\n",
      " [  0.           0.           0.           1.27037362  42.55751617]\n",
      " [ 35.66969859   0.          10.19134245   0.          12.73917807]\n",
      " [  8.30344095   7.74987822   0.           1.10712546  10.51769186]\n",
      " [  0.           0.           0.           1.34772706  32.3454494 ]\n",
      " [  5.42947117   0.           5.42947117  43.43576936   5.42947117]\n",
      " [ 12.73236567   0.           6.36618284   0.          12.73236567]\n",
      " [  0.           0.           0.           0.          18.51371867]\n",
      " [ 13.35113485   0.           0.          40.05340454   8.01068091]\n",
      " [ 16.72310715   4.18077679  12.54233036   0.           0.        ]\n",
      " [ 14.80275331   0.          11.10206498  14.80275331  18.50344164]\n",
      " [ 14.44112833  16.84798306  12.03427361   0.           7.22056417]\n",
      " [  0.           0.           0.           0.          31.06072371]\n",
      " [  0.           0.           0.           0.          16.41267545]\n",
      " [  0.           0.           0.           1.08391684  26.01400421]\n",
      " [  0.           0.           0.           1.40125365  28.0250731 ]\n",
      " [  0.           0.           0.          30.25604175  15.12802088]\n",
      " [  2.37397177   0.           0.           4.74794355  21.36574596]\n",
      " [  0.           0.           0.          13.97917104   8.38750262]\n",
      " [ 12.18302383   3.48086395  12.18302383   1.74043198  27.8469116 ]\n",
      " [  0.           0.           0.           0.           3.80966894]\n",
      " [  0.           0.           0.           0.          38.94586525]\n",
      " [  0.           0.           0.           0.          20.15410136]\n",
      " [  0.           0.           0.           0.          35.42519085]\n",
      " [  0.           0.           0.           3.99323812  17.30403184]\n",
      " [  0.           0.           0.           3.99291922  21.29556919]\n",
      " [  0.           0.           0.           0.51343136  92.93107697]\n",
      " [  6.11870283   2.03956761   0.           4.07913522  22.43524373]\n",
      " [  0.           0.           0.           2.949809    19.91121075]\n",
      " [  0.           0.           0.           1.27236985  36.2625408 ]\n",
      " [ 16.90539128   9.22112252   4.61056126   0.          35.34763632]\n",
      " [  0.           0.           0.           5.21711911  22.60751613]\n",
      " [  0.           0.           0.           0.          16.91761123]\n",
      " [  0.           0.           0.           0.          32.21977277]\n",
      " [  0.           0.           0.           2.66492558  23.98433024]\n",
      " [  0.           0.           0.           1.91532355  21.06855901]\n",
      " [  0.           0.           0.           0.          22.14304406]\n",
      " [ 16.52551126   4.13137781   6.19706672   2.06568891  26.85395579]\n",
      " [  0.           0.           0.           1.38809844  33.31436265]\n",
      " [  0.           0.           0.           1.33122113  21.29953807]\n",
      " [  0.           0.           0.           1.34323747  32.2376993 ]\n",
      " [  0.           0.           0.           0.          72.95198043]\n",
      " [  9.58975047  13.42565066   5.75385028   0.          17.26155085]\n",
      " [ 13.64682029  17.5459118    0.           3.89909151  17.5459118 ]\n",
      " [ 17.93110868   7.17244347   0.           0.          35.86221736]\n",
      " [ 12.82071828  17.62848764   3.20517957   0.          41.66733441]\n",
      " [  7.52752925  15.0550585    1.07536132   0.          62.37095664]\n",
      " [ 11.49007066   8.617553     0.           0.          77.55797698]\n",
      " [  4.5216542   16.27795513   0.           0.          55.16418126]\n",
      " [  0.           0.           0.           3.75481085  35.67070309]\n",
      " [  0.           0.           0.           0.          23.21034824]\n",
      " [ 16.60853679   9.96512207   0.           3.32170736  23.2519515 ]\n",
      " [  0.           0.           0.           0.           0.        ]\n",
      " [ 25.73391136   0.          27.713443     3.95906329   5.93859493]\n",
      " [  4.99467235   0.          16.64890783  58.27117741   1.66489078]\n",
      " [  0.           0.           0.           1.01566666   2.03133332]\n",
      " [ 16.14866711  19.25418002   0.           4.34771807   4.96882065]\n",
      " [  0.           0.           0.           2.18219114  40.37053606]\n",
      " [  8.78537333   0.97615259  10.73767851   0.97615259  17.57074666]\n",
      " [ 11.5614574    4.95491032  11.5614574   24.77455158   8.25818386]\n",
      " [  0.           0.           0.           0.          38.61252333]\n",
      " [  4.73496058   2.36748029   0.           2.36748029  66.28944814]\n",
      " [  0.           0.           0.           2.20911481  30.92760731]\n",
      " [  9.53061711   9.53061711   0.          57.18370264  47.65308554]\n",
      " [ 15.82998595   3.95749649   7.91499298   7.91499298   5.93624473]\n",
      " [  1.93532155  21.28853709  15.48257243  19.35321554  11.61192932]\n",
      " [ 52.22613918  24.24785033  14.92175405   3.73043851  33.57394662]\n",
      " [  0.           0.           0.           0.          37.99780782]\n",
      " [  6.75253642   3.37626821   8.44067053   0.          23.63387748]\n",
      " [  0.           0.           0.         112.4269225    7.49512817]\n",
      " [104.87793993   2.23144553   4.46289106   4.46289106  11.15722765]\n",
      " [  0.          11.51311055   0.          78.19320918  13.43196231]\n",
      " [  4.6972615    2.34863075   2.34863075   0.          32.88083048]\n",
      " [  0.           0.           0.           1.6639212   21.63097556]\n",
      " [  0.           2.56957114   0.          15.41742683   0.        ]\n",
      " [ 73.50238883   0.           0.           0.          36.75119441]\n",
      " [ 34.32003432   0.          12.87001287   8.58000858   4.29000429]\n",
      " [ 14.85133811   2.97026762  17.82160573   0.          17.82160573]\n",
      " [ 19.34508107   0.           8.79321867  45.72473708   1.75864373]\n",
      " [  3.49088878   1.74544439  10.47266634   1.74544439   1.74544439]\n",
      " [ 34.98833722  13.32889037  41.65278241   1.6661113    1.6661113 ]\n",
      " [ 34.98833722  13.32889037  41.65278241   1.6661113    1.6661113 ]\n",
      " [ 14.57938475   0.          16.19931639  16.19931639   4.85979492]\n",
      " [ 22.72469038   0.          19.47830604   9.73915302   0.        ]\n",
      " [ 14.17077357   6.29812159   6.29812159   1.5745304    6.29812159]\n",
      " [  0.           0.           0.          11.21051098  42.59994171]\n",
      " [  0.           0.           0.           2.55349574   7.66048721]\n",
      " [  0.           0.           1.20033609   1.20033609   8.40235266]\n",
      " [ 12.54215558  13.93572842   0.           0.          25.08431116]\n",
      " [  8.14190172  17.44693225   0.           1.16312882  54.66705438]\n",
      " [ 10.22320668   9.08729483  38.62100301  10.22320668  10.22320668]\n",
      " [  0.           0.           0.           8.43403978  27.71184501]\n",
      " [  0.           0.           0.           0.          75.11801799]\n",
      " [  0.          20.91612633   0.           0.           5.22903158]\n",
      " [  0.           0.           0.           1.91857565  65.23157208]\n",
      " [  5.73460259   1.43365065   7.16825324   0.          15.77015713]\n",
      " [  0.           0.           0.           3.42477482  56.50878455]\n",
      " [  0.           0.           0.           0.          26.94288109]\n",
      " [  0.           0.           0.           4.00133378  22.67422474]\n",
      " [  0.           0.           0.           0.          25.78161255]\n",
      " [ 10.74883554  21.49767109   3.58294518   7.16589036  32.24650663]\n",
      " [  8.04758807  14.75391146  11.40074977   0.67063234  26.15466123]\n",
      " [  0.           0.           0.           0.          54.48742897]\n",
      " [  0.           0.           0.           4.42041051   8.84082102]\n",
      " [  0.           0.           0.           1.8113645   18.11364501]\n",
      " [  0.           0.           0.           0.          15.6575033 ]\n",
      " [  0.           0.           0.           2.85095222  11.40380887]\n",
      " [  0.           0.           0.           0.78461526  19.6153816 ]\n",
      " [  0.           0.           0.           1.86550922   6.84020048]\n",
      " [  8.69414015  56.51191097   0.           0.          69.5531212 ]\n",
      " [  1.42900013   8.57400077   1.42900013  14.29000129   7.14500064]\n",
      " [  0.           0.           0.           0.           7.45489787]\n",
      " [  1.73070267   1.73070267   0.           0.          41.53686397]\n",
      " [  0.           0.           0.           4.7856049   27.11842777]\n",
      " [  0.           0.          12.00792523   4.00264174  18.01188785]\n",
      " [  0.           2.66155648   2.66155648   0.          26.61556478]\n",
      " [  0.           0.           0.           6.91818098   0.        ]\n",
      " [  0.           0.           0.           1.66992302   1.66992302]\n",
      " [ 20.65731579   2.06573158   0.           0.           2.06573158]\n",
      " [  0.           0.           0.           0.           5.76252629]\n",
      " [  0.           0.           0.           1.97523061  25.67799791]\n",
      " [  0.           0.           0.           1.54811942  23.22179133]\n",
      " [  0.           0.           0.           0.           9.80272025]\n",
      " [  0.           0.           0.           3.59513937  19.77326654]]\n"
     ]
    }
   ],
   "source": [
    "wc = corpus['wc'].to_numpy() #done during section- with teammates\n",
    "normalized = count/wc[:,None] * 100000\n",
    "\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the normalized term frequencies you just calculated for any three documents\n",
    "wc = corpus['wc'].to_numpy()\n",
    "wc = wc[:,None] \n",
    "# print(wc)\n",
    "t = (count.toarray())\n",
    "corpus['color'] = 0\n",
    "corpus['honor'] = 0\n",
    "corpus['center'] = 0\n",
    "corpus['fish'] = 0\n",
    "corpus['person'] = 0 \n",
    "#print(t)\n",
    "pd.set_option('mode.chained_assignment', None) #the error is actually not an error it is a warning (warning due to poor code structure, but still right)\n",
    "#can comment out to see error- it will have no effect on the code \n",
    "for i in range(len(corpus)): \n",
    "    corpus['color'][i]= t[i][0]\n",
    "    corpus['honor'][i] = t[i][1]\n",
    "    corpus['center'][i] = t[i][2]\n",
    "    corpus['fish'][i] = t[i][3]\n",
    "    corpus['person'][i] = t[i][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>country</th>\n",
       "      <th>wc</th>\n",
       "      <th>color</th>\n",
       "      <th>honor</th>\n",
       "      <th>center</th>\n",
       "      <th>fish</th>\n",
       "      <th>person</th>\n",
       "      <th>color_norm</th>\n",
       "      <th>honor_norm</th>\n",
       "      <th>center_norm</th>\n",
       "      <th>fish_norm</th>\n",
       "      <th>person_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little_Women_Alcott.txt</td>\n",
       "      <td>us</td>\n",
       "      <td>185902</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>11.296274</td>\n",
       "      <td>15.599617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.765425</td>\n",
       "      <td>12.372110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The_Great_God_Pan.txt</td>\n",
       "      <td>gb</td>\n",
       "      <td>21639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.242571</td>\n",
       "      <td>13.863857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The_Lost_Kafoozalum.txt</td>\n",
       "      <td>gb</td>\n",
       "      <td>22371</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.470073</td>\n",
       "      <td>26.820437</td>\n",
       "      <td>8.940146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NORTHANGER_ABBEY.txt</td>\n",
       "      <td>us</td>\n",
       "      <td>77059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.954139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>persuasion.txt</td>\n",
       "      <td>gb</td>\n",
       "      <td>83278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.425515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename country      wc  color  honor  center  fish  \\\n",
       "0  Little_Women_Alcott.txt      us  185902     21     29       0     7   \n",
       "1    The_Great_God_Pan.txt      gb   21639      0      0       0     2   \n",
       "2  The_Lost_Kafoozalum.txt      gb   22371      0      0       1     6   \n",
       "3     NORTHANGER_ABBEY.txt      us   77059      0      0       0     0   \n",
       "4           persuasion.txt      gb   83278      0      0       0     0   \n",
       "\n",
       "   person  color_norm  honor_norm  center_norm  fish_norm  person_norm  \n",
       "0      23   11.296274   15.599617     0.000000   3.765425    12.372110  \n",
       "1       3    0.000000    0.000000     0.000000   9.242571    13.863857  \n",
       "2       2    0.000000    0.000000     4.470073  26.820437     8.940146  \n",
       "3      20    0.000000    0.000000     0.000000   0.000000    25.954139  \n",
       "4      32    0.000000    0.000000     0.000000   0.000000    38.425515  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['color_norm'] = 0.0\n",
    "corpus['honor_norm'] = 0.0\n",
    "corpus['center_norm'] = 0.0\n",
    "corpus['fish_norm'] = 0.0\n",
    "corpus['person_norm'] = 0.0 \n",
    "pd.set_option('mode.chained_assignment', None) #same as above\n",
    "for i in range(len(corpus)): \n",
    "    corpus['color_norm'][i] = (corpus['color'][i]/wc[i]) * 100000\n",
    "    corpus['honor_norm'][i] = (corpus['honor'][i]/wc[i]) * 100000\n",
    "    corpus['center_norm'][i] = (corpus['center'][i]/wc[i]) * 100000\n",
    "    corpus['fish_norm'][i] = (corpus['fish'][i]/wc[i]) * 100000\n",
    "    corpus['person_norm'][i] = (corpus['person'][i]/wc[i]) * 100000\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "Both of the values found by each of the two methods I presented above are the same, but I felt that although the first method was much easier to do, the second method provides a visual aid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate analytic means and 95% confidence intervals (15 points)\n",
    "\n",
    "* For each of the five indicated terms, calculate and display the mean and 95% confidence interval within each national group.\n",
    "*  I suggest using the `tconfint_mean()` method from the `DescrStatsW()` function provided by the `statsmodels` library. See lecture notes for an example of working code.\n",
    "* Format your output (roughly) as follows:\n",
    "\n",
    "```\n",
    "Confidence intervals for: gb\n",
    "     term\t    low\t    mean\t    high\n",
    "   color\t  x.xxxx\t  x.xxxx\t  x.xxxx\n",
    "   [and so on ...]\n",
    "```\n",
    "\n",
    "In this part of the problem, calculate your means and CIs analytically, using the observed statistics of each sample, rather than by bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wc</th>\n",
       "      <th>color</th>\n",
       "      <th>honor</th>\n",
       "      <th>center</th>\n",
       "      <th>fish</th>\n",
       "      <th>person</th>\n",
       "      <th>color_norm</th>\n",
       "      <th>honor_norm</th>\n",
       "      <th>center_norm</th>\n",
       "      <th>fish_norm</th>\n",
       "      <th>person_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>131.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>79314.694656</td>\n",
       "      <td>3.648855</td>\n",
       "      <td>2.625954</td>\n",
       "      <td>2.061069</td>\n",
       "      <td>4.335878</td>\n",
       "      <td>20.908397</td>\n",
       "      <td>6.898563</td>\n",
       "      <td>3.721598</td>\n",
       "      <td>3.577896</td>\n",
       "      <td>6.362920</td>\n",
       "      <td>23.747075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54486.012043</td>\n",
       "      <td>6.783377</td>\n",
       "      <td>5.720860</td>\n",
       "      <td>5.080514</td>\n",
       "      <td>15.180762</td>\n",
       "      <td>28.769495</td>\n",
       "      <td>13.935690</td>\n",
       "      <td>7.608938</td>\n",
       "      <td>7.732523</td>\n",
       "      <td>15.343624</td>\n",
       "      <td>18.704961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2721.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47876.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.621587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60099.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.548119</td>\n",
       "      <td>21.295569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92321.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>10.486021</td>\n",
       "      <td>3.428566</td>\n",
       "      <td>4.022918</td>\n",
       "      <td>4.040888</td>\n",
       "      <td>32.613140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>331988.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>104.877940</td>\n",
       "      <td>56.511911</td>\n",
       "      <td>41.652782</td>\n",
       "      <td>112.426923</td>\n",
       "      <td>92.931077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  wc       color       honor      center        fish  \\\n",
       "count     131.000000  131.000000  131.000000  131.000000  131.000000   \n",
       "mean    79314.694656    3.648855    2.625954    2.061069    4.335878   \n",
       "std     54486.012043    6.783377    5.720860    5.080514   15.180762   \n",
       "min      2721.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%     47876.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%     60099.000000    0.000000    0.000000    0.000000    1.000000   \n",
       "75%     92321.500000    5.000000    2.000000    1.000000    3.000000   \n",
       "max    331988.000000   47.000000   31.000000   34.000000  163.000000   \n",
       "\n",
       "           person  color_norm  honor_norm  center_norm   fish_norm  \\\n",
       "count  131.000000  131.000000  131.000000   131.000000  131.000000   \n",
       "mean    20.908397    6.898563    3.721598     3.577896    6.362920   \n",
       "std     28.769495   13.935690    7.608938     7.732523   15.343624   \n",
       "min      0.000000    0.000000    0.000000     0.000000    0.000000   \n",
       "25%      5.000000    0.000000    0.000000     0.000000    0.000000   \n",
       "50%     13.000000    0.000000    0.000000     0.000000    1.548119   \n",
       "75%     25.500000   10.486021    3.428566     4.022918    4.040888   \n",
       "max    218.000000  104.877940   56.511911    41.652782  112.426923   \n",
       "\n",
       "       person_norm  \n",
       "count   131.000000  \n",
       "mean     23.747075  \n",
       "std      18.704961  \n",
       "min       0.000000  \n",
       "25%       8.621587  \n",
       "50%      21.295569  \n",
       "75%      32.613140  \n",
       "max      92.931077  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display analytic means and CIs\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color gb\n",
      "Confidence Interval: (0.05734754735849712, 1.204246452081989) Mean: 0.6307969997202431\n",
      "honor gb\n",
      "Confidence Interval: (-0.2501225410326629, 1.102709793545379) Mean: 0.42629362625635797\n",
      "center gb\n",
      "Confidence Interval: (-0.007464714372635295, 1.1855245885963273) Mean: 0.5890299371118458\n",
      "fish gb\n",
      "Confidence Interval: (1.717683649779449, 6.703498791980557) Mean: 4.210591220880003\n",
      "person gb\n",
      "Confidence Interval: (22.378088325893255, 31.80637495662347) Mean: 27.092231641258362\n",
      "\n",
      "color us\n",
      "Confidence Interval: (8.639749919684554, 17.131614347797083) Mean: 12.885682133740813\n",
      "honor us\n",
      "Confidence Interval: (4.601434723833549, 9.137269677288321) Mean: 6.869352200560936\n",
      "center us\n",
      "Confidence Interval: (4.050608922728463, 8.815255118870965) Mean: 6.432932020799715\n",
      "fish us\n",
      "Confidence Interval: (3.7917039117609717, 13.046048516897141) Mean: 8.418876214329059\n",
      "person us\n",
      "Confidence Interval: (16.13440211575759, 24.969001015479595) Mean: 20.551701565618597\n"
     ]
    }
   ],
   "source": [
    "gb = corpus.loc[corpus['country']=='gb'] \n",
    "us = corpus.loc[corpus['country']=='us'] \n",
    "for i in terms: \n",
    "    gb_i = sms.DescrStatsW(gb[i+'_norm']).tconfint_mean()\n",
    "    gb_mean = gb[i+'_norm'].mean()\n",
    "    print(i , 'gb')\n",
    "    print('Confidence Interval:', gb_i , 'Mean:', gb_mean)\n",
    "print('')\n",
    "for i in terms: \n",
    "    us_i = sms.DescrStatsW(us[i+'_norm']).tconfint_mean()\n",
    "    us_mean = us[i+'_norm'].mean()\n",
    "    print(i , 'us')\n",
    "    print('Confidence Interval:', us_i , 'Mean:', us_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate bootstrapped means and 95% confidence intervals (15 points)\n",
    "\n",
    "* Calculate the same quantities as above, but this time by bootrap resampling of your data. \n",
    "* Use a minimum of 1,000 trials for each case. \n",
    "* Format your results as in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb: color \n",
      " Mean: 0.21875 \n",
      " 95% CI: ( 0.0625 , 0.421875 )\n",
      "gb: honor \n",
      " Mean: 0.140625 \n",
      " 95% CI: ( 0.015625 , 0.375 )\n",
      "gb: center \n",
      " Mean: 0.234375 \n",
      " 95% CI: ( 0.046875 , 0.65625 )\n",
      "gb: fish \n",
      " Mean: 2.46875 \n",
      " 95% CI: ( 1.359375 , 4.0625 )\n",
      "gb: person \n",
      " Mean: 27.6875 \n",
      " 95% CI: ( 20.125 , 38.265625 )\n",
      "\n",
      "us: color \n",
      " Mean: 6.850746268656716 \n",
      " 95% CI: ( 5.134328358208955 , 9.014925373134329 )\n",
      "us: honor \n",
      " Mean: 4.925373134328358 \n",
      " 95% CI: ( 3.462686567164179 , 6.746268656716418 )\n",
      "us: center \n",
      " Mean: 3.7313432835820897 \n",
      " 95% CI: ( 2.283582089552239 , 5.462686567164179 )\n",
      "us: fish \n",
      " Mean: 5.880597014925373 \n",
      " 95% CI: ( 2.5223880597014925 , 11.761194029850746 )\n",
      "us: person \n",
      " Mean: 14.119402985074627 \n",
      " 95% CI: ( 10.582089552238806 , 18.776119402985074 )\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap calculations\n",
    "#from lecture\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "gb = corpus.loc[corpus['country']=='gb'] \n",
    "us = corpus.loc[corpus['country']=='us'] \n",
    "\n",
    "# bootstrapped_means_gb = []\n",
    "trials = 1000\n",
    "for i in terms:\n",
    "    bootstrapped_means_gb = []\n",
    "    k = gb[i].count()\n",
    "    for x in range(trials):\n",
    "        sample = gb[i].sample(n=k, replace=True) #create sample- code given by TA\n",
    "        bootstrapped_means_gb.append(np.mean(sample))\n",
    "    result_gb = sorted(bootstrapped_means_gb)\n",
    "    low_gb = result_gb[int(trials*0.025)] \n",
    "    high_gb = result_gb[int(trials*0.975)] \n",
    "    print('gb:',i ,'\\n', \"Mean:\", result_gb[int(trials/2)],'\\n' ,\"95% CI:\", '(',low_gb, ',' ,high_gb, ')')\n",
    "print('')\n",
    "for i in terms:\n",
    "    bootstrapped_means_us = []\n",
    "    k = us[i].count()\n",
    "    for x in range(trials):\n",
    "        sample = us[i].sample(n=k, replace=True) #create sample \n",
    "        bootstrapped_means_us.append(np.mean(sample))\n",
    "    result_us = sorted(bootstrapped_means_us)\n",
    "    low_us = result_us[int(trials*0.025)] \n",
    "    high_us = result_us[int(trials*0.975)] \n",
    "    print('us:',i ,'\\n', \"Mean:\", result_us[int(trials/2)],'\\n' ,\"95% CI:\", '(',low_us, ',' ,high_us, ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *t*-tests (20 points)\n",
    "\n",
    "* Perform a *t*-test comparing the mean frequency of each of the indicated terms in the British and American subsets of the corpus.\n",
    "    * You will perform 5 total tests, comparing, for example, the mean frequency of `color` in British texts to the mean frequency of `color` in American texts. Do not cross-compare words (that is, don't compare the frequency of `color` to that of `honor`, etc.).\n",
    "* Note that the *t*-test takes as input two lists of values. These values are the normalized counts for the feature in question in each volume of a subcorpus. There should thus be one list per subcorpus for each feature. You can produce these lists on the fly as you iterate over your feature data.\n",
    "* Display the test statistic and *p*-value for each comparison. \n",
    "    * Format your output for easy readability (do not just print the raw `ttest_ind` object).\n",
    "* Note which differences are significant at the *p*<0.05 level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "color_norm:\n",
      "t-statistic: -5.710855578615899\n",
      "p-value:     2.66232957951557e-07\n",
      "\n",
      "honor_norm:\n",
      "t-statistic: -5.435939141902402\n",
      "p-value:     6.095694670555516e-07\n",
      "\n",
      "center_norm:\n",
      "t-statistic: -4.751214779484977\n",
      "p-value:     9.648991593476994e-06\n",
      "\n",
      "fish_norm:\n",
      "t-statistic: -1.59890142906163\n",
      "p-value:     0.11296901234604427\n",
      "\n",
      "person_norm:\n",
      "t-statistic: 2.0223115383604093\n",
      "p-value:     0.04522772184736863\n"
     ]
    }
   ],
   "source": [
    "# Perform t-tests\n",
    "# code from lecture\n",
    "from scipy.stats import ttest_ind\n",
    "terms_norm = ['color_norm', 'honor_norm', 'center_norm', 'fish_norm', 'person_norm']\n",
    "for col in terms_norm:\n",
    "    print(f'\\n{col}:')\n",
    "    result = ttest_ind(\n",
    "        gb[col], \n",
    "        us[col],\n",
    "        equal_var=False\n",
    "    )\n",
    "    print('t-statistic:', result[0])\n",
    "    print('p-value:    ', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (25 points)\n",
    "\n",
    "* Vectorize the corpus as indicated below (freebie)\n",
    "* Standard-scale the resulting feature matrix\n",
    "* Produce a one-dimensional label vector, y, indicating the national origin of each volume in the corpus\n",
    "    * Use `1` to indicate American, `0` for British\n",
    "* Calculate the 10-fold cross-validated classification accuracy and F1 score using a logistic regression classifier on the full input matrix\n",
    "* From the full matrix, select the 25 most-informative features\n",
    "    * Use sklearn's `SelectKBest` function with the  `mutual_info_classif` scoring function to produce a feature matrix that contains just these 25 most-informative features\n",
    "    * Print a list of the names (token labels; for example, 'color') of these 25 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (131, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize (freebie)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pre_proc(x):\n",
    "    '''\n",
    "    Takes a unicode string.\n",
    "    Lowercases, strips accents, and removes some escapes.\n",
    "    Returns a standardized version of the string.\n",
    "    '''\n",
    "    import unicodedata\n",
    "    return unicodedata.normalize('NFKD', x.replace(\"_\", \" \").lower().strip())\n",
    "\n",
    "# Set up vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    preprocessor=pre_proc,\n",
    "    min_df=11, # Note this\n",
    "    max_df=0.8, # This, too\n",
    "    binary=False,\n",
    "    norm='l2',\n",
    "    max_features=5000,\n",
    "    use_idf=True # And this\n",
    ")\n",
    "\n",
    "# Perform vectorization\n",
    "X = vectorizer.fit_transform(filenames) # <-- MODIFY TO USE THE LIST OF FILES ON YOUR MACHINE\n",
    "\n",
    "# Get the dimensions of the doc-term matrix\n",
    "print(\"Matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard-scale your feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_standard = scaler.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.942707647121589e-18\n"
     ]
    }
   ],
   "source": [
    "# Print the overall mean of your scaled features (use np.mean(X)).\n",
    "# Should be very close to zero.\n",
    "print(np.mean(X_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a one-dimensional vector of true labels for classification\n",
    "# 1='us', 0='gb'\n",
    "y = []\n",
    "for i in range(len(corpus)): \n",
    "    if corpus['country'][i] == 'us': \n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using your label vector, display the number of US texts in the corpus\n",
    "display(np.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie function to summarize and display classifier scores\n",
    "def compare_scores(scores_dict, color=True):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean)\n",
    "    if color:\n",
    "        df = df.style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate the logistic regression classifier on full input data\n",
    "# Consult PS 6 for useful code- code below from pset\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifiers = {\n",
    "    'Logit':LogisticRegression()\n",
    "}\n",
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X_standard, # feature matrix\n",
    "        y, # actual labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_67e1a404_1f0c_11eb_891e_f01898156049row0_col0 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_67e1a404_1f0c_11eb_891e_f01898156049row0_col1 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_67e1a404_1f0c_11eb_891e_f01898156049row0_col2 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_67e1a404_1f0c_11eb_891e_f01898156049row0_col3 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_67e1a404_1f0c_11eb_891e_f01898156049row0_col4 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_67e1a404_1f0c_11eb_891e_f01898156049row0_col5 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_67e1a404_1f0c_11eb_891e_f01898156049\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >fit_time</th>        <th class=\"col_heading level0 col1\" >score_time</th>        <th class=\"col_heading level0 col2\" >test_accuracy</th>        <th class=\"col_heading level0 col3\" >test_f1</th>        <th class=\"col_heading level0 col4\" >test_f1_macro</th>        <th class=\"col_heading level0 col5\" >test_f1_micro</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_67e1a404_1f0c_11eb_891e_f01898156049level0_row0\" class=\"row_heading level0 row0\" >Logit</th>\n",
       "                        <td id=\"T_67e1a404_1f0c_11eb_891e_f01898156049row0_col0\" class=\"data row0 col0\" >0.073678</td>\n",
       "                        <td id=\"T_67e1a404_1f0c_11eb_891e_f01898156049row0_col1\" class=\"data row0 col1\" >0.002364</td>\n",
       "                        <td id=\"T_67e1a404_1f0c_11eb_891e_f01898156049row0_col2\" class=\"data row0 col2\" >0.847802</td>\n",
       "                        <td id=\"T_67e1a404_1f0c_11eb_891e_f01898156049row0_col3\" class=\"data row0 col3\" >0.843337</td>\n",
       "                        <td id=\"T_67e1a404_1f0c_11eb_891e_f01898156049row0_col4\" class=\"data row0 col4\" >0.846109</td>\n",
       "                        <td id=\"T_67e1a404_1f0c_11eb_891e_f01898156049row0_col5\" class=\"data row0 col5\" >0.847802</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb3cea7a340>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display your cross-validation results\n",
    "# Use the compare_scores function defined above\n",
    "compare_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 25 most-informative features as specified above \n",
    "#  and produce a new feature matrix containing only those features\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "best_25= SelectKBest(score_func = mutual_info_classif, k = 25)\n",
    "new_feat = best_25.fit_transform(X_standard,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 25)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of your new feature matrix\n",
    "print(new_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afterwards', 'brief', 'color', 'colored', 'colour', 'coloured', 'daisy', 'duly', 'england', 'events', 'extraordinary', 'grey', 'hats', 'honor', 'honour', 'honoured', 'humour', 'london', 'neighbourhood', 'notes', 'recognise', 'thereby', 'toward', 'towards', 'upwards']\n"
     ]
    }
   ],
   "source": [
    "# Get the names of the features retained in the new feature matrix\n",
    "# Store these feature names in a list, then print the list\n",
    "\n",
    "# Hint: use a combination of your original vectorizer's `.get_feature_names()` method \n",
    "#  and the `SelectKBest` object's `.get_support()` method\n",
    "\n",
    "#code from: https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "\n",
    "mask = best_25.get_support()\n",
    "names = vectorizer.get_feature_names()\n",
    "new = []\n",
    "\n",
    "for bool, feature in zip(mask, names):\n",
    "    if bool:\n",
    "        new.append(feature)\n",
    "\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_70e99598_1f0c_11eb_891e_f01898156049row0_col0 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_70e99598_1f0c_11eb_891e_f01898156049row0_col1 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_70e99598_1f0c_11eb_891e_f01898156049row0_col2 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_70e99598_1f0c_11eb_891e_f01898156049row0_col3 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_70e99598_1f0c_11eb_891e_f01898156049row0_col4 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_70e99598_1f0c_11eb_891e_f01898156049row0_col5 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_70e99598_1f0c_11eb_891e_f01898156049\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >fit_time</th>        <th class=\"col_heading level0 col1\" >score_time</th>        <th class=\"col_heading level0 col2\" >test_accuracy</th>        <th class=\"col_heading level0 col3\" >test_f1</th>        <th class=\"col_heading level0 col4\" >test_f1_macro</th>        <th class=\"col_heading level0 col5\" >test_f1_micro</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_70e99598_1f0c_11eb_891e_f01898156049level0_row0\" class=\"row_heading level0 row0\" >Logit</th>\n",
       "                        <td id=\"T_70e99598_1f0c_11eb_891e_f01898156049row0_col0\" class=\"data row0 col0\" >0.010948</td>\n",
       "                        <td id=\"T_70e99598_1f0c_11eb_891e_f01898156049row0_col1\" class=\"data row0 col1\" >0.003978</td>\n",
       "                        <td id=\"T_70e99598_1f0c_11eb_891e_f01898156049row0_col2\" class=\"data row0 col2\" >0.924176</td>\n",
       "                        <td id=\"T_70e99598_1f0c_11eb_891e_f01898156049row0_col3\" class=\"data row0 col3\" >0.918022</td>\n",
       "                        <td id=\"T_70e99598_1f0c_11eb_891e_f01898156049row0_col4\" class=\"data row0 col4\" >0.922643</td>\n",
       "                        <td id=\"T_70e99598_1f0c_11eb_891e_f01898156049row0_col5\" class=\"data row0 col5\" >0.924176</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb3cf353a30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display the 10-fold cross-validated accuracy and F1 of the\n",
    "#  logistic regression using the new, smaller feature matrix\n",
    "new_scores = {} \n",
    "for classifier in classifiers: \n",
    "    new_scores[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        new_feat, # feature matrix\n",
    "        y, # actual labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )\n",
    "compare_scores(new_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_719e1162_1f0c_11eb_891e_f01898156049row0_col0 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row0_col1 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row0_col2 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row0_col3 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row0_col4 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row0_col5 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row1_col0 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row1_col1 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row1_col2 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row1_col3 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row1_col4 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_719e1162_1f0c_11eb_891e_f01898156049row1_col5 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_719e1162_1f0c_11eb_891e_f01898156049\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >fit_time</th>        <th class=\"col_heading level0 col1\" >score_time</th>        <th class=\"col_heading level0 col2\" >test_accuracy</th>        <th class=\"col_heading level0 col3\" >test_f1</th>        <th class=\"col_heading level0 col4\" >test_f1_macro</th>        <th class=\"col_heading level0 col5\" >test_f1_micro</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_719e1162_1f0c_11eb_891e_f01898156049level0_row0\" class=\"row_heading level0 row0\" >old</th>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row0_col0\" class=\"data row0 col0\" >0.097114</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row0_col1\" class=\"data row0 col1\" >0.004100</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row0_col2\" class=\"data row0 col2\" >0.847802</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row0_col3\" class=\"data row0 col3\" >0.843337</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row0_col4\" class=\"data row0 col4\" >0.846109</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row0_col5\" class=\"data row0 col5\" >0.847802</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_719e1162_1f0c_11eb_891e_f01898156049level0_row1\" class=\"row_heading level0 row1\" >new</th>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row1_col0\" class=\"data row1 col0\" >0.008194</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row1_col1\" class=\"data row1 col1\" >0.003508</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row1_col2\" class=\"data row1 col2\" >0.924176</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row1_col3\" class=\"data row1 col3\" >0.918022</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row1_col4\" class=\"data row1 col4\" >0.922643</td>\n",
       "                        <td id=\"T_719e1162_1f0c_11eb_891e_f01898156049row1_col5\" class=\"data row1 col5\" >0.924176</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb3d0f9aca0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_scores = {}\n",
    "all_feats = {\n",
    "    'old': X_standard,\n",
    "    'new': new_feat\n",
    "}\n",
    "for i in all_feats: \n",
    "    c_scores[i] = cross_validate(\n",
    "        LogisticRegression(), \n",
    "        all_feats[i], \n",
    "        y, \n",
    "        cv=10, \n",
    "        scoring=['accuracy', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )\n",
    "\n",
    "compare_scores(c_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the 5 most important features (15 points)\n",
    "\n",
    "* Split the new matrix of most-informative features into train (75%) and test (25%) sets (use sklearn's `train_test_split`)\n",
    "* Train a default logistic regression classifier on the training set\n",
    "    * Print the trained model's score on the test set (use the trained classifier's `.score()` method)\n",
    "* Use sklearn's `permutation_importance` function to calculate the importance of each input feature\n",
    "* Print the feature importances from most to least important using the supplied function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the selected feature matrix into train and test sets\n",
    "# Then, train a logistic regression classifier on the train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_feat, y, test_size=0.25, random_state=42)\n",
    "model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "# Print the score of the trained classifier on the test set\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'importances_mean': array([ 0.01221374, -0.00152672,  0.03358779,  0.00763359,  0.00305344,\n",
      "        0.01374046,  0.00152672,  0.01374046,  0.01526718,  0.0351145 ,\n",
      "        0.00152672,  0.02442748,  0.01374046,  0.02137405,  0.01679389,\n",
      "       -0.00152672,  0.        ,  0.00305344,  0.01068702,  0.00305344,\n",
      "        0.        ,  0.01832061,  0.0610687 ,  0.01832061,  0.06717557]), 'importances_std': array([0.01142491, 0.00305344, 0.00778476, 0.00682769, 0.00778476,\n",
      "       0.0101271 , 0.00305344, 0.01480208, 0.01365538, 0.0103547 ,\n",
      "       0.00747936, 0.01313332, 0.00890222, 0.01221374, 0.01121904,\n",
      "       0.00571245, 0.0048279 , 0.00610687, 0.0103547 , 0.01142491,\n",
      "       0.        , 0.00778476, 0.01365538, 0.00778476, 0.01556953]), 'importances': array([[ 0.00763359,  0.02290076, -0.00763359,  0.02290076,  0.01526718],\n",
      "       [ 0.        ,  0.        , -0.00763359,  0.        ,  0.        ],\n",
      "       [ 0.03053435,  0.03053435,  0.03816794,  0.02290076,  0.04580153],\n",
      "       [ 0.01526718,  0.01526718,  0.        ,  0.        ,  0.00763359],\n",
      "       [-0.00763359,  0.        ,  0.        ,  0.00763359,  0.01526718],\n",
      "       [ 0.00763359,  0.01526718,  0.03053435,  0.        ,  0.01526718],\n",
      "       [ 0.00763359,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.02290076,  0.03816794,  0.        ,  0.00763359,  0.        ],\n",
      "       [ 0.03053435,  0.02290076,  0.00763359, -0.00763359,  0.02290076],\n",
      "       [ 0.03053435,  0.05343511,  0.02290076,  0.03053435,  0.03816794],\n",
      "       [ 0.00763359,  0.00763359, -0.00763359, -0.00763359,  0.00763359],\n",
      "       [ 0.04580153,  0.01526718,  0.00763359,  0.03053435,  0.02290076],\n",
      "       [ 0.02290076,  0.        ,  0.02290076,  0.01526718,  0.00763359],\n",
      "       [ 0.03816794,  0.02290076,  0.00763359,  0.00763359,  0.03053435],\n",
      "       [ 0.02290076,  0.03053435,  0.02290076,  0.        ,  0.00763359],\n",
      "       [ 0.        ,  0.00763359, -0.00763359, -0.00763359,  0.        ],\n",
      "       [ 0.        ,  0.        , -0.00763359,  0.        ,  0.00763359],\n",
      "       [ 0.00763359, -0.00763359,  0.00763359,  0.00763359,  0.        ],\n",
      "       [ 0.01526718,  0.01526718, -0.00763359,  0.00763359,  0.02290076],\n",
      "       [ 0.        ,  0.02290076, -0.00763359, -0.00763359,  0.00763359],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.01526718,  0.02290076,  0.03053435,  0.01526718,  0.00763359],\n",
      "       [ 0.04580153,  0.07633588,  0.07633588,  0.04580153,  0.0610687 ],\n",
      "       [ 0.03053435,  0.00763359,  0.02290076,  0.01526718,  0.01526718],\n",
      "       [ 0.05343511,  0.06870229,  0.04580153,  0.08396947,  0.08396947]])}\n"
     ]
    }
   ],
   "source": [
    "# Calculate feature importance via permutation\n",
    "from sklearn.inspection import permutation_importance\n",
    "p_feature = permutation_importance(model, new_feat, y)\n",
    "print(p_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie function to print ranked list of feature importances\n",
    "def print_importances(object_importance, feature_names):\n",
    "    '''\n",
    "    Takes a trained permutation_importance object and a list of feature names.\n",
    "    Prints an ordered list of features by descending importance.\n",
    "    '''\n",
    "    for i in object_importance.importances_mean.argsort()[::-1]:\n",
    "\n",
    "        print(f\"{feature_names[i]:<8}\"\n",
    "            f\"\\t{object_importance.importances_mean[i]:.3f}\"\n",
    "            f\" +/- {object_importance.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upwards \t0.067 +/- 0.016\n",
      "toward  \t0.061 +/- 0.014\n",
      "events  \t0.035 +/- 0.010\n",
      "color   \t0.034 +/- 0.008\n",
      "grey    \t0.024 +/- 0.013\n",
      "honor   \t0.021 +/- 0.012\n",
      "towards \t0.018 +/- 0.008\n",
      "thereby \t0.018 +/- 0.008\n",
      "honour  \t0.017 +/- 0.011\n",
      "england \t0.015 +/- 0.014\n",
      "coloured\t0.014 +/- 0.010\n",
      "duly    \t0.014 +/- 0.015\n",
      "hats    \t0.014 +/- 0.009\n",
      "afterwards\t0.012 +/- 0.011\n",
      "neighbourhood\t0.011 +/- 0.010\n",
      "colored \t0.008 +/- 0.007\n",
      "london  \t0.003 +/- 0.006\n",
      "notes   \t0.003 +/- 0.011\n",
      "colour  \t0.003 +/- 0.008\n",
      "extraordinary\t0.002 +/- 0.007\n",
      "daisy   \t0.002 +/- 0.003\n",
      "humour  \t0.000 +/- 0.005\n",
      "recognise\t0.000 +/- 0.000\n",
      "honoured\t-0.002 +/- 0.006\n",
      "brief   \t-0.002 +/- 0.003\n"
     ]
    }
   ],
   "source": [
    "# Print ranked list of features by permutation importance\n",
    "print_importances(p_feature, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 \n",
    "1. upwards\n",
    "2. toward\n",
    "3. events\n",
    "4. color\n",
    "5. grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
