{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 6: Classification\n",
    "\n",
    "## Description\n",
    "\n",
    "Explore different algorithms to classify Old Bailey cases as involving \"stealing\" or \"other.\"\n",
    "\n",
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from   sklearn.decomposition import TruncatedSVD\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Our input text file\n",
    "old_bailey_file = os.path.join('..', '..', 'data', 'old_bailey', 'old_bailey.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Bailey records\n",
    "\n",
    "We'll work with a set of 3,090 short text documents from the Old Bailey archive, just as we did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cases in as a list of strings\n",
    "with open(old_bailey_file, 'r') as f:\n",
    "    bailey = [doc for doc in f.read().split('\\n\\n')] # split on consecutive newlines\n",
    "print(\"Total documents:\", len(bailey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie function to assign gold labels to corpus texts\n",
    "def make_labels(corpus, keyword='steal'):\n",
    "    '''\n",
    "    Takes a corpus of documents and a keyword string.\n",
    "    Assigns labels based on coöccurrence of keyword and ' indicted for ' in first sentence of document.\n",
    "    Returns an array of class labels (1=member, 0=nonmember).\n",
    "    '''\n",
    "    from nltk import word_tokenize, sent_tokenize\n",
    "    import re\n",
    "    find_indictment = re.compile(' indicted for ')\n",
    "    find_keyword = re.compile(keyword)\n",
    "    labels = []\n",
    "    for doc in corpus:\n",
    "        first_sentence = sent_tokenize(doc)[0].lower()\n",
    "        if find_indictment.search(first_sentence) and find_keyword.search(first_sentence):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate gold labels\n",
    "y_bailey = make_labels(bailey, keyword='steal')\n",
    "print(\"Label vector shape:\", y_bailey.shape)\n",
    "print(\"Stealing cases:\", np.sum(y_bailey))\n",
    "print(\"Fraction of cases labeled 'stealing':\", round(np.sum(y_bailey)/len(y_bailey),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy and F1\n",
    "#  What if we just guess 1 every time?\n",
    "baseline_accuracy = np.sum(y_bailey)/len(y_bailey)\n",
    "baseline_precision = baseline_accuracy\n",
    "baseline_recall = 1.0\n",
    "baseline_f1 = 2*baseline_precision*baseline_recall/(baseline_precision+baseline_recall)\n",
    "print(\"Baseline accuracy:\", round(baseline_accuracy, 3))\n",
    "print(\"Baseline F1:\", round(baseline_f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorize (5 points)\n",
    "\n",
    "Using the vectorizer defined below, transform the input documents into a TFIDF-weighted document-term matrix. Store your vectorized output in a varaible named `X_bailey` and print the shape of the resulting matrix.\n",
    "\n",
    "Note: This is a straight carry-over from the last problem set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing to remove escaped characters in input\n",
    "def pre_proc(x):\n",
    "    '''\n",
    "    Takes a unicode string.\n",
    "    Lowercases, strips accents, and removes some escapes.\n",
    "    Returns a standardized version of the string.\n",
    "    '''\n",
    "    import unicodedata\n",
    "    return unicodedata.normalize('NFKD', x.replace(\"\\'\", \"'\").replace(\"\\ in\\ form\", \" inform\").lower().strip())\n",
    "\n",
    "# Set up vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    encoding='utf-8',\n",
    "    preprocessor=pre_proc,\n",
    "    min_df=2, # Note this\n",
    "    max_df=0.8, # This, too\n",
    "    binary=False,\n",
    "    norm='l2',\n",
    "    use_idf=True # And this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the doc-term matrix\n",
    "print(\"Matrix shape:\", X_bailey.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie: plot our data\n",
    "coords_bailey = TruncatedSVD(n_components=2).fit_transform(X_bailey)\n",
    "\n",
    "plt.subplots(figsize=(12,8))\n",
    "sns.scatterplot(\n",
    "    x=coords_bailey[:, 0], \n",
    "    y=coords_bailey[:, 1],\n",
    "    hue=y_bailey,\n",
    "    alpha=0.2,\n",
    "    linewidth=0\n",
    ")\n",
    "plt.title('Old Bailey Theft vs. Other')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freebies: Classification and cross-validation how-to\n",
    "\n",
    "Below is a sample of how to set up classifiers and perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate two simple classifiers on our data\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Classifers to test\n",
    "classifiers = {\n",
    "    'kNN': KNeighborsClassifier(),\n",
    "    'Logit':LogisticRegression()\n",
    "}\n",
    "\n",
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X_bailey, # feature matrix\n",
    "        y_bailey, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the performance of our simple classifiers\n",
    "# Freebie function to summarize and display classifier scores\n",
    "def compare_scores(scores_dict):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean).style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare cross-validation scores\n",
    "# Note that colorization of the `time` columns is counterintuitive\n",
    "compare_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add results from three more classifiers (25 points)\n",
    "\n",
    "* Set up the three classifiers imported below (decision tree, random forest, and multinomial naïve Bayes). Use **default** parameters only (that is, do not set any classifier parameters yourself). (10 points total)\n",
    "* Cross-validate the three new classifiers, saving the scoring output to the same `scores` dictionary as above. (10 points)\n",
    "* Use the `compare_scores` function to display the scores of all five classifiers. (5 points)\n",
    "\n",
    "This all takes less than 30 seconds total to run on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare the unoptimized performance of the five classifiers (10 points)\n",
    "\n",
    "How would you summarize the performance of the five classifiers prior to any optimization? Consider classification performance relative to one another and to baseline, as well as computation time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comparison here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improve two of the classifiers (40 points)\n",
    "\n",
    "See how much you can improve the performance of any two of the five classifiers. You might try:\n",
    "\n",
    "* Changing the vectorization parameters. (Hint: it can help to use fewer input features).\n",
    "* Changing the classifier parameters. \n",
    "    * See the `sklearn` [documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) for your chosen classifiers for a list of available options.\n",
    "* Trying a different classifier entirely.\n",
    "    * If you do this, make sure you give careful consideration to the settings of the new classifier, too. **Using default parameters will result in a score of zero.**\n",
    "    \n",
    "No matter what you try, you must **document your process**. In other words, don't just change settings in a cell and run it over and over. You should iterate over different options, storing your cross-validation scores for each combination of settings.\n",
    "\n",
    "Finally, display your results using the `compare_scores` function.\n",
    "\n",
    "Points breakdown: 20 points total for each classifier, of which 10 are for iterating over a range of parameter values, 5 are for performing cross validation correctly and displaying summary scores, and 5 are for actually improving performance (by any amount). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine and discuss classification errors (20 points)\n",
    "\n",
    "First, rerun your best-performing classifier over the full corpus and save the resulting labels as `y_best`. (10 points)\n",
    "\n",
    "To do this, first set up a classifier with your optimized paramters; then use the classifier's `fit` method to train the model; then use the fitted classifer's `predict` method to compute the labels. In schematic form, this looks like:\n",
    "\n",
    "```\n",
    "y_labels = Classifier(options).fit(X, y).predict(X)\n",
    "```\n",
    "\n",
    "(Note that training and testing over the same data is poor practice, since it encourages overfitting. We avoided that problem by using cross validation above. We're doing it here only for ease of examination.)\n",
    "\n",
    "Then use the `pull_errors` function below to read through some cases that were incorrectly classified by your best-perfoming classifer. Write a paragraph that summarizes any patterns you can identify in the mis-classified cases. Can you explain what may be confusing the classifier? Does your analysis suggest any avenues for improved performance? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to examine some classification errors\n",
    "def pull_errors(labels, gold_labels=y_bailey, corpus=bailey, n=3):\n",
    "    '''\n",
    "    Takes:\n",
    "        an array of computed labels\n",
    "        an array of gold (correct) labels\n",
    "        a list of corpus documents\n",
    "        an int of cases to display\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    labeled_cases = pd.DataFrame(\n",
    "        {\n",
    "            'gold':gold_labels, \n",
    "            'computed':labels,\n",
    "            'text':corpus\n",
    "        }   \n",
    "    )\n",
    "    errors = labeled_cases.loc[labeled_cases.gold != labeled_cases.computed]\n",
    "    with pd.option_context('display.max_colwidth', None):\n",
    "        display(errors.sample(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull errors\n",
    "pull_errors(y_best, y_bailey, bailey, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your discussion here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
