{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**30 November 2020**\n",
    "\n",
    "# Word embeddings\n",
    "\n",
    "## Admin\n",
    "\n",
    "* Problem set 11 (the last one!)\n",
    "    * To be released later this week\n",
    "    * Due Tuesday, 12/8, at 11:59pm\n",
    "* Read Nelson for Wednesday\n",
    "    * Fourth and final reading response due by the evening of Tuesday, 12/15\n",
    "* Need to discuss readings and/or lecture topics for next week\n",
    "    * Post your ideas on Campuswire (threads already exist there) by Wednesday evening\n",
    "* Exam and final project description and guidelines coming this week\n",
    "    * Exam due date TBD this week\n",
    "    * Exam released one week before due date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words as vectors\n",
    "\n",
    "* We've previously represented **documents** as vectors\n",
    "    * Usually, vectors of word counts\n",
    "    * Or, with topic models, as vectors of topics\n",
    "* Limits of words, often discussed\n",
    "    * In brief: some words are more alike than others, but word counts don't reflect this fact\n",
    "    * Topic models helped to address this problem\n",
    "    * Today, a different approach: **word embeddings**\n",
    "* We might like to have a vector for each **word**, which we could in turn use to build our document vectors\n",
    "    * This will be an *unsupervised* task. We want to produce vectors that place similar words close together in vector space, but we don't want to use human-labled data to do so. We want instead to observe patterns of use in a large corpus.\n",
    "* One approach (not pursued here): Truncated SVD on sliding windows over words\n",
    "    * Group all the resulting bags of words by their 'center' word\n",
    "    * Reduce resulting \"doc\"-term matrix for each word via truncated SVD\n",
    "        * Like latent semantic analysis, but oriented around words rather than documents\n",
    "    * Relatively fast to train and decent performance\n",
    "    * Limited by linear relationships and lack of semantic connections\n",
    "* Language models\n",
    "    * Large neural language models achieve state-of-the-art performance on many (all?) NLP tasks\n",
    "    * These models generally seek to predict a word (or other element of a text sequence) on the basis of the words around it\n",
    "    * General architecture is:\n",
    "        * Embedding layer. Counts of words transformed into contextual vectors. Initially, weights are random. In the case of word embeddings, we want to learn the proper weights as our output objective.\n",
    "        * Intermediate (hidden) layer. Neural layers that can learn non-linear relationships between words and embeddings.\n",
    "        * Output layer. Optimize an objective function (for example, highest probability of next word, given an input sequence).\n",
    "    * `word2vec` (2013) showed that we can do without the intermediate layer if we just want to produce word embeddings\n",
    "        * This simplifies the computation, so we can use *much* more input text\n",
    "* CBOW vs. skip-gram\n",
    "    * `word2vec` can use two different models: Continuous bag of words (CBOW) and skip-gram\n",
    "    * CBOW is more \"traditional.\" Objective is to predict a word given the *n* words before and after it.\n",
    "    * Skip-gram \"flips\" the prediction task. Try to predict the *n* contextual words, given a seed word.\n",
    "    * CBOW is faster to train and better captures *syntax* (plurals are closer to other plurals in vector space)\n",
    "    * Skip-gram is slower to train and better captures *semantic* relationships ('king' moves  closer to 'prince' than to 'kings')\n",
    "* Training is slow either way, so using pretrained embeddings is standard practice, but ...\n",
    "    * Embedded relationships and biases of training set\n",
    "    * Change over time\n",
    "    * Polysemy remains within the embedding\n",
    "        * \"Class\" vector has components of education and of social status, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's *use* some embeddings\n",
    "\n",
    "We're not going to train them, because that's slow and requires lots of corpus text. If we have time, we'll at least mention transfer learning in a future lecture. Transfer learning allows us to use a modest amount of new training data to update a larger, pretrained model.\n",
    "\n",
    "### We need a `spaCy` medium or large model\n",
    "\n",
    "The small models that we used in the previous lecture do not include vectors. Uncomment and run the cell below to download and install the large model, which includes `GloVe` embeddings (`GloVe` is an alternative to `word2vec`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") # Note '_lg' = large model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king -> queen similarity: 0.7252609729766846\n"
     ]
    }
   ],
   "source": [
    "# How similar are the words 'king' and 'queen' in Web text?\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"king -> queen similarity:\", cosine_similarity(\n",
    "    nlp.vocab['king'].vector.reshape(1,-1), # reshape into a 2d vector with single row\n",
    "    nlp.vocab['queen'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman -> queen similarity: 0.7880845069885254\n"
     ]
    }
   ],
   "source": [
    "# Vector math: king - man + woman -> closer to queen?\n",
    "king_to_queen_vector = nlp.vocab['king'].vector - nlp.vocab['man'].vector + nlp.vocab['woman'].vector\n",
    "print(\"king - man + woman -> queen similarity:\", cosine_similarity(\n",
    "    king_to_queen_vector.reshape(1,-1), \n",
    "    nlp.vocab['queen'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris -> Berlin:                    0.5989030599594116\n",
      "Paris - France + Germany -> Berlin: 0.7547403573989868\n"
     ]
    }
   ],
   "source": [
    "# Try the same with Paris and Berlin\n",
    "print(\"Paris -> Berlin:                   \", cosine_similarity(\n",
    "    nlp.vocab['Paris'].vector.reshape(1,-1), # reshape into a 2d vector with single row\n",
    "    nlp.vocab['Berlin'].vector.reshape(1,-1)\n",
    ").item())\n",
    "\n",
    "paris_to_berlin_vector = nlp.vocab['Paris'].vector - nlp.vocab['France'].vector + nlp.vocab['Germany'].vector\n",
    "print(\"Paris - France + Germany -> Berlin:\", cosine_similarity(\n",
    "    paris_to_berlin_vector.reshape(1,-1), \n",
    "    nlp.vocab['Berlin'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can use vectors on these kinds of \"analogy\" tasks. How could we find the *n* most similar words to a given vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus-wide similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1476295\n",
      "Has vectors: 684754\n",
      "CPU times: user 37.3 s, sys: 592 ms, total: 37.9 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# How large is our model?\n",
    "string_count = 0\n",
    "vector_count = 0\n",
    "for string in nlp.vocab.strings:\n",
    "    string_count+=1\n",
    "    if nlp.vocab[string].has_vector:\n",
    "        vector_count+=1\n",
    "print(\"Vocab size:\", string_count)\n",
    "print(\"Has vectors:\", vector_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix of all our embeddings. This matrix will have 684,754 rows (one for each word that has an embedding in our model) and 300 columns (the number of dimensions in the embedding representation).\n",
    "\n",
    "We initialize the output matrix with zeros, since it's much faster to update an existing numpy matrix than it is to append to it.\n",
    "\n",
    "Note that there's an easier way to do this in spaCy, but we're showing the details for clarity ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (684754, 300)\n",
      "CPU times: user 17.5 s, sys: 960 ms, total: 18.5 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make a word-vector martrix with labels\n",
    "import numpy as np\n",
    "vector_matrix = np.zeros([vector_count,nlp.vocab.vectors_length]) # Initialize the output matrix\n",
    "counter = 0\n",
    "vocab_dict = {} # Dictionary to hold word index positions in the matrix\n",
    "vocab_list = [] # List to hold words in order\n",
    "for string in nlp.vocab.strings: # iterate over the strings in our model\n",
    "    if nlp.vocab[string].has_vector: # only want the ones with embeddings\n",
    "        vocab_dict[string] = counter # record positiuon of this word\n",
    "        vocab_list.append(string) # add to our list of words\n",
    "        # normalize the vector and update matrix\n",
    "        vector_matrix[counter] = nlp.vocab[string].vector/nlp.vocab[string].vector_norm\n",
    "        counter+=1 # increment counter\n",
    "print(\"Matrix shape:\", vector_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to iterate over the rows in the output matrix, calculating the similarity between each one and your target vector. This is slow. Instead, we'll use matrix math.\n",
    "\n",
    "The **dot product** of two 1D vectors is the sum of their element-wise products. If the vectors are (L2) normalized, their dot product = their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684754,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarities to 'king' via dot products\n",
    "similarities = np.dot(vector_matrix, vector_matrix[vocab_dict['king']])\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king 1.000000026584885\n",
      "KIng 1.000000026584885\n",
      "KING 1.000000026584885\n",
      "King 1.000000026584885\n",
      "KINGS 0.787661407451134\n",
      "Kings 0.787661407451134\n",
      "kings 0.787661407451134\n",
      "prince 0.7337737081547175\n",
      "Prince 0.7337737081547175\n",
      "PRINCE 0.7337737081547175\n"
     ]
    }
   ],
   "source": [
    "# Get 10 most-similar words to 'king'\n",
    "# argsort returns lowest to highest, so we get the last 10, then reverse\n",
    "top_n = np.argsort(similarities)[-10:][::-1] \n",
    "for i in top_n:\n",
    "    print(vocab_list[i], similarities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the underlying vectors are obviously not case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king     \t0.8024\n",
      "queen     \t0.7881\n",
      "prince     \t0.6401\n",
      "kings     \t0.6209\n",
      "princess     \t0.6126\n",
      "royal     \t0.5801\n",
      "throne     \t0.5787\n"
     ]
    }
   ],
   "source": [
    "# Most similar to our king - man + woman vector\n",
    "# Also, make this prettier and with less repetition\n",
    "\n",
    "def print_most_similar(matrix, vector, vocab_list, n=20):\n",
    "    sim = np.dot(matrix, vector/np.linalg.norm(vector))\n",
    "    sorted_sims = np.argsort(sim)[-n:][::-1] #argsort return lowest to highest, so we get the last n, then reverse\n",
    "    last_word = ''\n",
    "    for i in sorted_sims:\n",
    "        current_word = vocab_list[i].lower()\n",
    "        if current_word != last_word:\n",
    "            print(f\"{current_word}     \\t{sim[i]:.4}\")\n",
    "            last_word = current_word\n",
    "            \n",
    "print_most_similar(vector_matrix, king_to_queen_vector, vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure spacy\n",
    "\n",
    "Do the same thing, but entirely within spacy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = nlp.vocab.vectors.most_similar(king_to_queen_vector.reshape(1,-1), n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[13176088972490086564, 14826469074451677028,  7464393751932445219,\n",
       "          7102492827649024548, 10168488388102651113,  4176741725343376093,\n",
       "          5247273317732208552, 12278543830867659210, 11742085837932180620,\n",
       "          4527521648030784477,  5578855234836721309,  3708855936003160130,\n",
       "          6599815902708227193,  8637828561746775750, 13482751041672084183,\n",
       "         10227307316083268331,  9482215189757771475, 10275277408386923532,\n",
       "           982181824006872346, 13488378799514498820]], dtype=uint64),\n",
       " array([[391588,   2183,   3150,  27270,   5310,   6026,  59856,  11900,\n",
       "          94889,   7474,   6603,  58142,   9576,   9118,  91941,   8298,\n",
       "          95982,   7927,   9741,  19962]], dtype=int32),\n",
       " array([[0.8024, 0.8024, 0.8024, 0.8024, 0.7881, 0.7881, 0.7881, 0.6401,\n",
       "         0.6401, 0.6401, 0.6209, 0.6209, 0.6209, 0.6126, 0.6126, 0.6126,\n",
       "         0.5801, 0.5801, 0.5801, 0.5787]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the output\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output format of spacy's `most_similar()` function is a tuple that contains three items: an array of word hashes (unique IDs), an array of index positions in the vector matrix, and an array of similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king     \t0.8024\n",
      "queen     \t0.7881\n",
      "prince     \t0.6401\n",
      "kings     \t0.6209\n",
      "princess     \t0.6126\n",
      "royal     \t0.5801\n",
      "throne     \t0.5787\n"
     ]
    }
   ],
   "source": [
    "# Print top matches\n",
    "hashes, lines, scores = best\n",
    "last_word = ''\n",
    "for i in range(len(hashes[0])):\n",
    "    current_word = nlp.vocab[hashes[0][i].item()].text.lower()\n",
    "    if current_word != last_word:\n",
    "        print(f'{current_word}     \\t{scores[0][i]:.4}')\n",
    "        last_word = current_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarities using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 1: The baby kitten is tinier than the elderly hound.\n",
      "   Built-in similarity: 0.923\n",
      "   Hand similarity:     0.923\n",
      "   Embed sim, no stops: 0.782\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "   Built-in similarity: 0.789\n",
      "   Hand similarity:     0.789\n",
      "   Embed sim, no stops: 0.492\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 3: Some paragraphs concern a cat and its traits.\n",
      "   Built-in similarity: 0.849\n",
      "   Hand similarity:     0.849\n",
      "   Embed sim, no stops: 0.639\n",
      "   Token sim, no stops: 0.224\n",
      "\n",
      "Doc 1: The baby kitten is tinier than the elderly hound.\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "   Built-in similarity: 0.719\n",
      "   Hand similarity:     0.719\n",
      "   Embed sim, no stops: 0.364\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 1: The baby kitten is tinier than the elderly hound.\n",
      "Doc 3: Some paragraphs concern a cat and its traits.\n",
      "   Built-in similarity: 0.783\n",
      "   Hand similarity:     0.783\n",
      "   Embed sim, no stops: 0.503\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "Doc 3: Some paragraphs concern a cat and its traits.\n",
      "   Built-in similarity: 0.86\n",
      "   Hand similarity:     0.86\n",
      "   Embed sim, no stops: 0.643\n",
      "   Token sim, no stops: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Doc similarities\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set up sample sentences\n",
    "sentences = [\n",
    "    'The young cat is smaller than the old dog.',\n",
    "    'The baby kitten is tinier than the elderly hound.',\n",
    "    'Not all sentences are about animals, nor about their properties.',\n",
    "    'Some paragraphs concern a cat and its traits.'\n",
    "]\n",
    "\n",
    "# process sentences into spacy nlp objects\n",
    "docs = []\n",
    "for sentence in sentences:\n",
    "    tokens = nlp(sentence)\n",
    "    docs.append(tokens)\n",
    "    \n",
    "def remove_noninformative_tokens(doc):\n",
    "    '''\n",
    "    Takes a spacy-processed document.\n",
    "    Returns a list of spacy token objects without stopwords, punctuation, or embedding-less tokens.\n",
    "    '''\n",
    "    culled = [token for token in doc if not (token.is_stop or token.is_punct) and token.has_vector]\n",
    "    return(culled)\n",
    "\n",
    "def embedding_similarity(X,Y):\n",
    "    '''\n",
    "    Takes two lists of spacy token objects.\n",
    "    Returns cosine similarity between their embedding representations.\n",
    "    '''\n",
    "    mean_vector_X = np.mean([token.vector for token in X], axis=0)\n",
    "    mean_vector_Y = np.mean([token.vector for token in Y], axis=0)\n",
    "        # note below reshape 1D -> 2D array. 2D version is 1 row x n columns\n",
    "    cos_similarity = cosine_similarity(mean_vector_X.reshape(1, -1), mean_vector_Y.reshape(1, -1))\n",
    "    return(cos_similarity.item()) # use .item() to get single value from array\n",
    "    \n",
    "def token_similarity(X,Y):\n",
    "    '''\n",
    "    Takes two lists of spacy tokens.\n",
    "    Returns cosine similarity between their tokens.\n",
    "    '''\n",
    "    doc1 = ' '.join([token.text for token in X])\n",
    "    doc2 = ' '.join([token.text for token in Y])\n",
    "    vectorizer = TfidfVectorizer(use_idf=False)\n",
    "    vectors = vectorizer.fit_transform([doc1, doc2])\n",
    "    return(cosine_similarity(vectors[0], vectors[1]).item())\n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "    doc1_nostops = remove_noninformative_tokens(doc)\n",
    "    for j in range(len(docs)):\n",
    "        if j>i:\n",
    "            doc2_nostops = remove_noninformative_tokens(docs[j])\n",
    "            print(f\"Doc {i}: {docs[i]}\")\n",
    "            print(f\"Doc {j}: {docs[j]}\")\n",
    "            print(f\"   Built-in similarity: {doc.similarity(docs[j]):.3}\")\n",
    "            print(f\"   Hand similarity:     {embedding_similarity(doc, docs[j]):.3}\")\n",
    "            print(f\"   Embed sim, no stops: {embedding_similarity(doc1_nostops, doc2_nostops):.3}\")\n",
    "            print(f\"   Token sim, no stops: {token_similarity(doc1_nostops, doc2_nostops):.3}\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
